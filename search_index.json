[["index.html", "R for PR evaluation General information on the course What can I learn from this tutorial? What can I do if I have a question? What can I do if I have problems with my R code?", " R for PR evaluation Lara Kobilke, IfKW, Ludwig-Maximilians-Universität München 2022-07-20 General information on the course This online tutorial will accompany the seminar “Theories, models, methods and objects of controlling”, an M.A. Seminar at the IfKW (Ludwig-Maximilians-Universität München, SS2022). The course takes place in Oettingenstraße 67, Room 067 (Wednesday 12:15-13.45 am). You can access all necessary information on the seminar’s syllabus, important dates and assignments via Moodle, “SoSe 22 Theorien, Modelle, Methoden und Gegenstände der PR-Wirkungs- und Erfolgskontrolle”. There, additional resources (e.g. Powerpoint slides, book resources, etc.) are uploaded to the material folders of the respective week. What can I learn from this tutorial? After completing the entire tutorial, you will have acquired two important skills: Know how to use R and R studio to complete your data management. Know how to use R and R studio to do digital media analysis. In the seminar, our sessions will consist of two parts. In the first part, we will learn about evaluation techniques and discuss your own experiences. In the second part, you will work through this tutorial and solve exercises together with fellow students. The tutorial is designed to provide the R skills that you need to put media resonance analysis and automated clipping into real-world practice. The goal is to make the individual seminar sessions diverse and to advance your programming skills with as much ease and fun as possible. Each tutorial consists of: an introduction to new functions/analysis methods in R, including corresponding R code main take-aways that you should remember information on additional tutorials / sources exercises to practice your new skills (you’ll often work through these with your peers) What can I do if I have a question? Questions are welcome! It’s natural to have a lot of questions because you’ll be learning a lot of new stuff. Please do ask if you don’t understand something in this tutorial, have questions concerning exercises, or simply need to talk through some of the new stuff. It’s quite natural to get lost at times when learning a new programming language. Don’t worry; it’s extremely probable that everyone else is experiencing the same thing. Therefore, it’s key that you ask questions: In our live sessions: Wednesday, 12:15-13:45. In the Moodle forum: Please, post all your questions to the Moodle forum. Every participant will be able to see your questions, submit replies, and view my responses in this manner. As a result, everyone will have access to the same information. To stay updated about current debates, I propose that you turn on alerts about new entries in the forum. Email: Send me an email at lara.kobilke@ifkw.lmu.de if you have any private questions that you don’t want to address in front of the rest of the class. What can I do if I have problems with my R code? Besides asking a question during the seminar (see What can I do if I have a question?), there are some great places to have a look at when you encounter problems. To give you a head start, here’s a quick rundown of the three best places to look if you have a problem with your code: R’s integrated help function: Use the ?-function whenever possible. Let’s assume you struggle with creating a histogram for your data (hist function in R). You can open the R documentation of the hist function in R by writing: ?hist Preview of ?hist in R: Search engines: Like Bing or Google. Yup, programmers and data scientists google all the time! Nobody knows all the code and errors by heart. Often you can find perfect answers to your questions on Stackoverflow, StatsExchange, or Rseek because other people had exactly the same problems. And more importantly, the communities on these websites are very friendly and helpful. Packages’ reference manuals: Finally, problems with R packages (we’ll get to packages later, see: Packages) can often be solved by looking at their reference manuals (an overview document containing all of a package’s functions). For example, you can learn more about dplyr (a data management package that we are going to use later in this tutorial) by visiting its reference manual on a website called “CRAN”: https://cran.r-project.org/web/packages/dplyr/dplyr.pdf. That is all there is to it. Let’s begin with our first tutorial: Tutorial: Installing &amp; Understanding R/R Studio "],["tutorial-installing-understanding-rr-studio.html", " 1 Tutorial: Installing &amp; Understanding R/R Studio 1.1 Installing R 1.2 Installing R Studio 1.3 Updating R and R Studio 1.4 How does R work? 1.5 Why should I use R? 1.6 How does R Studio work? 1.7 Packages 1.8 Take-Aways 1.9 Additional tutorials", " 1 Tutorial: Installing &amp; Understanding R/R Studio After working through Tutorial 1, you’ll… know how to install R and R Studio know how to update R and R Studio understand the layout of R Studio 1.1 Installing R R is the programming language we’ll use to import, edit, and analyze data. Please watch one of these two video tutorials before installing R yourself. Video Tutorial for Windows Video Tutorial for Mac When you are ready to install R, use Cran to install the newest version of R (4.1.2, “Bird Hippie”). You’ll have to specify your operation system to download the right version: Installer for Windows Installer for Mac 1.2 Installing R Studio Next, install R Studio. R Studio is a desktop application with a graphical interface that facilitates programming with R. The newest version of R Studio (1.4.1717) can be downloaded via this Link. 1.3 Updating R and R Studio If you have already installed R and RStudio (for example, because you already needed it for a previous seminar), please update your version to the latest version. This way, we’ll all know that our versions are compatible. 1.3.1 On Windows Updating on Windows is tricky. Therefore, you can use a package called installr, which helps you manage your update. First, install the installr package if you don’t have it. Use the following code: # installing/loading the package: if(!require(installr)) { install.packages(&quot;installr&quot;); require(installr) } #load / install+load installr After you have installed or loaded the installr package, let’s start the updating process of your R installation by using the updateR() function. It will check for newer versions, and if one is available, will guide you through the decisions you’d need to make: # using the package: updateR() Finally, update R Studio. Updating RStudio is easy, just open RStudio and go to Help &gt; Check for Updates to install a newer version. 1.3.2 On MAC Go to CRAN and install the newer package installer. After that update R Studio. Updating RStudio is easy, just open RStudio and go to Help &gt; Check for Updates to install a newer version. 1.4 How does R work? R is an object- and function-oriented programming language. Chambers (2014, p. 4) explains “object- and function-oriented” like this: Everything that exists is an object. Everything that happens is a function call. IN R, you will assign values (for instance, single numbers/letters, several numbers/letters, or whole data files) to objects in R to work with them. For example, this command will assign the letters “hello” to an object caled word by using the assign operator &lt;- (a function used to assign values to objects): word &lt;- &quot;hello&quot; The type of each object will dictate what sorts of computations you may be done with this object. The object word, for example, is distinguished by the fact that it is made up of characters (i.e., it is a word) - which may make it impossible to compute the object’s mean value, for example (which is possible only for objects consisting of numerical data). 1.5 Why should I use R? There are several reasons why I’m an advocate of R (or similar programming languages such as Python) over programs such as SPSS. R is free. Other than most other (statistical) programs, you do not need to buy it (or rely on an university license, that is likely to run out once you leave your department). R is an open source program. Other than most other programs, the source code - i.e., the basis of the program - is freely available. So are the hundred of packages (we’ll get to those later – these are basically additional functions you may need for more specific analyses) on CRAN that you can use to extend R’s base functions. R offers you flexibility. You can work with almost any type of data and rely on a large (!) set of functions to import, edit, or analyze such data. And if the function you need to do so hasn’t been implemented (or simply does not exist yet), you can write it yourself! Learning R increases your chances on the job market. For many jobs (academia, market research, data science, data journalism), applicants should know at least one programming language. 1.6 How does R Studio work? As mentioned, R studio is a graphical interface which facilitates programming with R. It contains up to four main windows, which allow for different things: Writing your own code (Window 1: Source). Important: When first installing R/R Studio and opening R studio, you may not see this window right away. In this case, simply open it by clicking on File/New File/R Script. Executing your own code (Window 2: Console) Inspecting objects (Window 3: Environment) Visualizing data, searching for help, updating packages etc. (Window 4: Files/Plots/Packages etc.) Image: Four main windows in R Please note that the specific set-up of your R Studio may look different (the order of windows may vary and so may the windows’ names). I have made the experience that having these four windows open works best for me. This may be different for you. If you want to modify the appearance of your R Studio, simply choose “Tools/Global Options/Pane Layout”. Image: Changing the Layout 1.6.1 Source: Writing your own code Using the window “Source”, you’ll write your own code to execute whichever task you want R to fulfill. 1.6.1.1 Writing Code Let’s start with an easy example: Assume you simply want R to print the word “hello”. In this case, you would first write a simple command that assigns the word “hello” to an object called word. The assigment of values to named objects is done via either the operator “&lt;-” or the operator “=”. The left side of that command contains the object that should be created; its right side the values that should be assigned to this object. In short, this command tells R to assign the world “hello” to an object called word. word &lt;- &quot;hello&quot; Image: “Source” 1.6.1.2 Annotating Code Another helpful aspect of R is that you can comment your own code. Oftentimes, this is very helpful for understanding your code later (if you write several hundred lines of codes, you may not remember their exact meaning months later). Comments or notes can be made via hashtags #. Anything following a hashtag will not be considered code by R but be ignored instead. word &lt;- &quot;hello&quot; #this line of code assigns the word &quot;hello&quot; to an object called word 1.6.1.3 Executing Code We now want to execute our code. Doing so is simple: Mark the parts of the code you want to run (for instance, single rows of code or blocks of code across several rows) Either press Run (see upper right side of the same window) or press Ctrl + Enter (On Mac OS X, hold the command key and press return instead). R should now execute exactly those lines of codes that you marked (hereby creating the object word). If you haven’t marked any specific code, all lines of code will be executed. Image: Executing Code 1.6.1.4 Saving Code A great feature of R is that it makes analyses easily reproducible - given that you save your code. When reopening R Studio and you script, you can simply “rerun” the code with one click and your analysis will be reproduced. To save code, you have two options: Choose the menu option File/Save as. Important: Code needs to be saved with the ending “.R”. Chose the Save-button in the source window and save your code in the correct format, for instance as “MyCode.R”. Image: Saving code 1.6.2 Console: Printing results Results of executing code are printed in a second window called “Console”, which includes the code you ran and the object you may have called when doing so. Previously, we defined an object called word, which consists of the single word “hello”. Thus, R prints our code as well as objects called when running this code (here, the object word) in the console. word &lt;- &quot;hello&quot; word ## [1] &quot;hello&quot; Image: Window “Console” 1.6.3 Environment: Overview of objects The third window is called “Environment”1. This windows displays all the objects currently existing - in our case, only the object “word”. As soon as you start creating more objects, this environment will fill up. If you’re an SPSS user, this window is very similar to what is called the Datenansicht / Data overview in SPSS. However, the R version of this is much more flexible, given that our environment can contain several data sets, for example, at the same time. Image: Window “Environment” It is important to know that we can visually inspect any object using the View() command (with a new tab then opening in the “Source” window). This isn’t super helpful right now - but if you work with bigger data sets with several observations/variables later on, it is often useful to inspect data visually. View(word) Image: Window “View” 1.6.4 Plots/Help/Packages: Do everything else Lastly, the standard R Studio interface contains a fourth window (if you opted for this layout). In my case, the window contains several sub-sections called “Files”, “Plots”, or “Packages” among others. You’ll understand their specific functions later - the window can, for instance, be used to plot/visualize results or see which packages are currently loaded. Image: Window “Files/Plots/Packages” 1.7 Packages While Base R, i.e., the standard version of R, already includes many helpful functions, you may at times need other, additional functions. For instance, if we want to perform text analysis in R we’ll need to use specific packages including additional functions. Packages are collections of topic-specific functions that extend the functions implemented in Base R. In the spirit of “open science”, anyone can write and publish these additional functions and related packages and anyone can also access the code used to do so. You’ll find a list of all of R packages here. In this seminar, we’ll for instance use packages like dplyr for advanced data management. 1.7.1 Installing packages To use a package, you have to install it first. Let’s say you’re interested in using the package dplyr. Using the command install.packages(), you can install the package on your computer. You’ll have to give the function the name of the package you are interested in installing. install.packages(&quot;dplyr&quot;) Now the package has been installed on your computer and is accessible locally. We only have to use install.packages() for any package once. Afterwards, the only thing you’ll have to do after open R is to activate the already installed package - which we’ll learn next. 1.7.2 Activating packages Before we are able to use a package, we need to activate it in each session. Thus, you should not only define a working directory at the beginning of each session but also activate the packages you want to use via the library()_ command. Again, you’ll have to give R the name of the package you want to activate: library(dplyr) You can also use the name of the package followed by two colons :: to activate a package directly before calling one of its function. For instance, I do not need use to activate the dplyr package (by using the library() function) to use the function summarize() if I use the following code: dplyr::summarize() 1.7.3 Getting information about packages The package is installed and activated - but how can we use it? To get an overview of functions included in a given package, you can consult its corresponding “reference manual” (overview document containing all of a package’s functions) or, if available, its “vignette” (tutorials on how to use selected functions for the corresponding package) provided by a package’s author on a website called “CRAN”. The easiest way to finding these manuals/vignettes is Google: Simply google CRAN ProcessR, for instance, and you’ll be guided to the following website: Image: Cran Overview dplyr package The first paragraph (circled in red) gives you an overview of aspects for which this package may be useful. The second red-circled area links to the reference manual and the vignette. You can, for instance, check out the reference manual to get an idea of the many functions the dplyr package contains. 1.8 Take-Aways Window “Source”: used to write/execute code in R Window “Console”: used to return results of executed code Window “Environment”: used to inspect objects on which to use functions Window “Files/Plots/Packages etc.”: used for additional functions, for instance visualizations/searching for help/activating or updating packages 1.9 Additional tutorials You still have questions? The following tutorials &amp; papers can help you with that: YaRrr! The Pirate’s Guide to R by N.D.Phillips, Tutorial 2 Computational Methods in der politischen Kommunikationsforschung by J. Unkel, Tutorial 1 SICSS Boot Camp by C. Bail, Video 1 wegweisR by M. Haim, Video 1 R Cookbook by Long et al., Tutorial 1 Now that you know the layout of R, we can get started with some real action: Tutorial: Using R as a calculator again, this only applies for the way I set up my R Studio. You can change this via “Tools/Global Options/Pane Layout”↩︎ "],["tutorial-using-r-as-a-calculator.html", " 2 Tutorial: Using R as a calculator 2.1 Using variables for calculation 2.2 Using vectors for calculation 2.3 Selecting values from a vector 2.4 Take-Aways 2.5 Additional tutorials", " 2 Tutorial: Using R as a calculator After working through Tutorial 2, you’ll… be able to work with mathematical operators in R be able to use mathematical operators on variables and vectors subset values from vectors One of the first things everyone learns in R is to use R as a calculator. You have access to many mathematical operators in R (e.g. +, -, *, /, ^). Let’s try some of them. Addition: 5+7 ## [1] 12 Subtraction: 12-7 ## [1] 5 Exponentiation: 3^3 ## [1] 27 2.1 Using variables for calculation You can also assign numbers to variables with the assign operator “&lt;-”. We have already talked about assigning word or numbers to variables in the chapter Writing Code. Please remember that a variable name in R can include numeric and alphabets along with special characters like dot (.) and underline (_).’ Therefore, these are good options to name your variables: my_1st_number &lt;- 3 my.1st.numer &lt;- 3 Do !not! use these variable names because they will cause errors and throw warning messages. I have therefore put the code as annotation to avoid the warning messages (with #): # _number &lt;- 3 # .number &lt;- 3 # my-1st-number &lt;- 3 You can use variables in your calculations by assigning the numbers to variables (i.e. store the numerical value in the variable).’ five &lt;- 5 seven &lt;- 7 twelve &lt;- five + seven # here you add the two variables in which the numbers are stored. The result of the addition is stored in the variable &quot;twelve&quot; twelve # now you have to retrieve the content of the variable, so that the result is printed to the console ## [1] 12 The names of the variables are freely selectable. For example, you can also proceed like this: three &lt;- 5 three # print the content of the variable to the console ## [1] 5 2.2 Using vectors for calculation You can also store more than one number in a variable. We call this process “creating vectors” because variables that contain more than one number are called “vectors” in R (we’ll get to vectors in [Tutorial: Objects &amp; structures in R]). Vectors are created using the combine function c() in R. twelve &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12) twelve # print the content of the variable to the console ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 Again, the variable name is chosen arbitrarily. You can also do this: twelve &lt;- c(4,10,15,21,33) twelve # print the content of the variable to the console ## [1] 4 10 15 21 33 You can use mathematical operations on vectors (e.g., +, -, * and /). Let’s create two vectors “weight” and “height” that contain the weight and height measures of 6 individuals. For example, the first individual weighs 60 kg and is 1.75 m tall: weight &lt;- c(60, 72, 57, 90, 95, 72) height &lt;- c(1.75, 1.80, 1.65, 1.90, 1.74, 1.91) Now we can calculate the Body Mass Index (BMI) using the BMI formula: BMI &lt;- weight/height^2 BMI # print the content of the BMI variable to the console ## [1] 19.59184 22.22222 20.93664 24.93075 31.37799 19.73630 Now we know that the first person has a BMI of 19.59, which is within the range of normality (18.5 and 24.9). 2.3 Selecting values from a vector We still see the BMI of all the other five people, i.e. the entire vector. How can we select only the first person? You can select values from a vector by using square brackets [ ] and enter the number of the entry that you want to print to your console. BMI[1] ## [1] 19.59184 Again, you can see that the first person has a BMI of 19.59. You could also decide to look at all values except the first one: BMI[-1] ## [1] 22.22222 20.93664 24.93075 31.37799 19.73630 You can even use the [ ] selector on vectors that consist of words instead of numbers. These vectors are called “character vectors”, while vectors that contain numbers are called “numeric vectors”. Let’s create a character vector that contains the BMIs of the six individuals as words. We’ll need to put quotation marks around your entries so that R knows that those values are words not numbers. BMI_word &lt;- c(&quot;nineteen&quot;, &quot;twenty-two&quot;, &quot;twenty&quot;, &quot;twenty-four&quot;, &quot;thirty-one&quot;, &quot;nineteen&quot;) BMI_word ## [1] &quot;nineteen&quot; &quot;twenty-two&quot; &quot;twenty&quot; &quot;twenty-four&quot; &quot;thirty-one&quot; ## [6] &quot;nineteen&quot; We’ll now select only the first value of of this BMI_word character vector: BMI_word[1] ## [1] &quot;nineteen&quot; If you want to select multiple values, you can index them. Let’s select the BMI of the third, fourth and fifth individual: BMI_word[3:5] ## [1] &quot;twenty&quot; &quot;twenty-four&quot; &quot;thirty-one&quot; 2.4 Take-Aways Mathematical operators: use +, -, *, /, ^ Use case: use these operators on numbers, variables, and vectors Create vectors: use the combine function c() Select values from vectors: use square brackets [ ] 2.5 Additional tutorials You still have questions? The following online guides can help you with that: Using R as a Calculator R Vector Let’s keep going: Tutorial: Working with data (files) "],["tutorial-working-with-data-files.html", " 3 Tutorial: Working with data (files) 3.1 Defining your working directory 3.2 Import data from your working directory 3.3 Subsetting variables / columns in data frames 3.4 Subsetting observations / rows in data frames 3.5 Subsetting values / cells in data frames 3.6 Subsetting data with conditions 3.7 Take-Aways 3.8 Additional tutorials", " 3 Tutorial: Working with data (files) After working through Tutorial 3, you’ll… understand how to import data know how to select variables in data frames know how to access values in data frames 3.1 Defining your working directory In most cases, you do not want to manually enter all your values into R and combine them with the “c()” function. Instead, you want to import data files that you already have on your drive/personal computer. The first step to importing your data into R is to define your working directory. Your working directory is the folder from which data can be imported into R or to which you can export and save data created with R. Create a folder that you want to use as your working directory for this tutorial (or use an existing one, that also works). For example, I’ve created a folder called “IPR”. Go to that folder and copy the path to it: Image: Working Directory on Windows Image: Copy Working Directory on Windows On Mac, you go to your folder and right click on it. An options menu opens and you can copy the folder path: Image: Copy Working Directory on MAC Now you know where this working directory is located - but R should know, too! Telling R from which folder to import data or where to export data to is also called setting your working directory. We call a function called setwd() (you guessed right: short for “setting your working directory”) which allows us to do exactly that. Important: The way this working directory is set differs between Windows- and Mac-Operating Systems. Windows: The dashes need to be pointing towards the right direction (if you simply copy the path to the folder, you may need to replace these signs “\\” with “/”) setwd(&quot;C:/Users/LaraK/Documents/IPR&quot;) Mac: You may need to add a “/” at the beginning like so: setwd(&quot;/Users/LaraK/Documents/IPR&quot;) If you have forgotten where you set your working directory, you can also ask R about the path of your current working directory with getwd(): getwd() ## [1] &quot;C:/Users/LaraK/Documents/IPR&quot; 3.1.1 Optional: Setting the working directory on a remote desktop The LMU Munich provides you with remote desktop access to the PCs in the local CIP-Pools. If you want to use your remote desktop to run R &amp; RStudio, you can follow this link to log into the remote desktop. This is a great fix if – for whatever reason – you can’t get R(Studio) installed on your machine and need a quick solution! Once you have logged in to the remote desktop, you can open RStudio and set the working directory (and import data) just like it is described in this tutorial (see next image). Image: Working Directory (&amp; Data Import) on a Remote Desktop The drawback: Since Windows does not allow RStudio to save script files without permission (and the IfKW for some reason has not given the permission), you can’t save script files on the remote desktop. You can solve this with a workaround: Paste your script into a text file before closing RStuio. E.g., WordPad is pre-installed. Save this file as .R. When you want to load your script in RStudio, right click on the .R file and choose “open with RStudio”. Image: Saving a Script File on a Remote Desktop 3.2 Import data from your working directory After setting the working directory, you need to transfer the data file that you want to work with to that folder (here: the “IPR” folder). Download the “data_tutorial3.csv” from Moodle, i.e., the 4. Mai material folder. The data set consists of data that is completely made up - a survey with 20 fictional students in a fictional seminar. We only use this data here so you can understand differences between types of data and types of objects. The data file data_tutorial3.csv is structured as follows: Each row contains the answer for a single student. Each column contains all values given by students for a single variable. The variables included here are: name: the name of each student age: the age of each student (in years) date: the date on which each student was surveyed (in YYYY-MM-DD) outlet: the type of media outlet each student’s mainly uses to get information outlet_use: the time each student uses this media outlet daily (in hours) outlet_trust: how much each student trusts this media outlet (from 1 = not at all to 5 = very much) We’ll read in the file with read.csv(). Here, we specify where to find the data file with the file path in quotation marks, but you don’t need to specify that path if you have already set it as your working directory. In addition, we provide an argument to let R know that the first row contains variable names with the argument header = TRUE. In the end, we assign our data file to a source object that we call survey. The data is now stored in this object. While read.csv() reads in comma-separated values, read.csv2() reads in values that are separated by semicolons. survey &lt;- read.csv2(&quot;data_tutorial3.csv&quot;, header = TRUE) survey ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 1 1 1 1 Alexandra 20 2021-09-09 TV 2 5 ## 2 2 2 2 Alex 25 2021-09-08 Online 3 5 ## 3 3 3 3 Maximilian 29 2021-09-09 Zeitung 4 1 ## 4 4 4 4 Moritz 22 2021-09-06 TV 2 2 ## 5 5 5 5 Vanessa 25 2021-09-07 Online 1 3 ## 6 6 6 6 Andrea 26 2021-09-09 Online 3 4 ## 7 7 7 7 Fabienne 26 2021-09-09 TV 3 2 ## 8 8 8 8 Fabio 27 2021-09-09 Online 0 1 ## 9 9 9 9 Magdalena 8 2021-09-08 Online 1 4 ## 10 10 10 10 Tim 26 2021-09-07 TV NA 2 ## 11 11 11 11 Alex 27 2021-09-09 Online NA 2 ## 12 12 12 12 Tobias 26 2021-09-07 Online 2 2 ## 13 13 13 13 Michael 25 2021-09-09 Online 3 2 ## 14 14 14 14 Sabrina 27 2021-09-08 Online 1 2 ## 15 15 15 15 Valentin 29 2021-09-09 TV 1 5 ## 16 16 16 16 Tristan 26 2021-09-09 TV 2 5 ## 17 17 17 17 Martin 21 2021-09-09 Online 1 2 ## 18 18 18 18 Anna 23 2021-09-08 TV 3 3 ## 19 19 19 19 Andreas 24 2021-09-09 TV 2 5 ## 20 20 20 20 Florian 26 2021-09-09 Online 1 5 3.3 Subsetting variables / columns in data frames In Tutorial: Using R as a calculator your variables where “floating” in your workspace / environment. They were not kept in a container, so you could call them by simply writing their name in the console. When you import data files to R, all variables in that data set a stored into a “container”, i.e. your source object. These containers for variables are called data frame in R. Variables that are part of a data frame can be accessed by their name, but we need to specify the data frame AND the variable name and combine them with the access operator: $. This takes the form of: dataframe$variablename # the first part is the container name, i.e. data frame # this is followed by the access operator $ # finally, you call the variable by name For instance, we could retrieve the variable “name” in our survey data frame by simply using its variable name: We specify the object we want to access, the data frame survey and then retrieve the column name via the operator $: survey$name ## [1] &quot;Alexandra&quot; &quot;Alex&quot; &quot;Maximilian&quot; &quot;Moritz&quot; &quot;Vanessa&quot; ## [6] &quot;Andrea&quot; &quot;Fabienne&quot; &quot;Fabio&quot; &quot;Magdalena&quot; &quot;Tim&quot; ## [11] &quot;Alex&quot; &quot;Tobias&quot; &quot;Michael&quot; &quot;Sabrina&quot; &quot;Valentin&quot; ## [16] &quot;Tristan&quot; &quot;Martin&quot; &quot;Anna&quot; &quot;Andreas&quot; &quot;Florian&quot; You know that the name variable is the fourth column of you data frame. Therefore, you can also access this column / variable by calling it by its index number (column index, here: 4). Just like you’ve learned in Tutorial: Using R as a calculator, you can access sub-elements of a greater object with square brackets [ ]: survey[4] ## name ## 1 Alexandra ## 2 Alex ## 3 Maximilian ## 4 Moritz ## 5 Vanessa ## 6 Andrea ## 7 Fabienne ## 8 Fabio ## 9 Magdalena ## 10 Tim ## 11 Alex ## 12 Tobias ## 13 Michael ## 14 Sabrina ## 15 Valentin ## 16 Tristan ## 17 Martin ## 18 Anna ## 19 Andreas ## 20 Florian Note: While the first command gives you the names as a vector, the second one gives you the name as a data frame object with only one column. This keeps the column header “name” intact. However, if you want to retrieve a vector using the column index, you need to provide two indices: one for the row that you want to select, followed by a comma, and one for the column. Since we want to select all rows, but only column No. 4, we need leave the row No. blank: survey[,4] # column index = 4 ## [1] &quot;Alexandra&quot; &quot;Alex&quot; &quot;Maximilian&quot; &quot;Moritz&quot; &quot;Vanessa&quot; ## [6] &quot;Andrea&quot; &quot;Fabienne&quot; &quot;Fabio&quot; &quot;Magdalena&quot; &quot;Tim&quot; ## [11] &quot;Alex&quot; &quot;Tobias&quot; &quot;Michael&quot; &quot;Sabrina&quot; &quot;Valentin&quot; ## [16] &quot;Tristan&quot; &quot;Martin&quot; &quot;Anna&quot; &quot;Andreas&quot; &quot;Florian&quot; 3.4 Subsetting observations / rows in data frames Using the same indexing technique, you can also select an entire row by providing a row index and leaving the column index blank: survey[1,] # row index = 1 ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 1 1 1 1 Alexandra 20 2021-09-09 TV 2 5 3.5 Subsetting values / cells in data frames You can subset values of a data set by calling a variable by its name. You just have to specify the data frame, the variable name AND the row index. For example, let’s look only at the first name in the data, which is Alexandra. survey$name[1] ## [1] &quot;Alexandra&quot; Any educated guesses on how to access the exact same value using the column index instead of the column name? Yeah, we enter the row index first, followed by a comma, and finish with the column index: survey[1,4] # row index = 1, column index = 4 ## [1] &quot;Alexandra&quot; Of course, you can use complex indexing on data frames. Let’s look at the first ten rows of the fourth (name) and fifth (age) column: survey[1:10,4:5] ## name age ## 1 Alexandra 20 ## 2 Alex 25 ## 3 Maximilian 29 ## 4 Moritz 22 ## 5 Vanessa 25 ## 6 Andrea 26 ## 7 Fabienne 26 ## 8 Fabio 27 ## 9 Magdalena 8 ## 10 Tim 26 3.6 Subsetting data with conditions Let’s say we want to select all names of students who are older than 25. This time, we’ll need to use a condition to select our rows: survey[survey$age &gt; 25,4] ## [1] &quot;Maximilian&quot; &quot;Andrea&quot; &quot;Fabienne&quot; &quot;Fabio&quot; &quot;Tim&quot; ## [6] &quot;Alex&quot; &quot;Tobias&quot; &quot;Sabrina&quot; &quot;Valentin&quot; &quot;Tristan&quot; ## [11] &quot;Florian&quot; # Read: I want to select column 4 (name) from the survey data frame # and display all rows in which the age column of the survey data frame has a value &gt; 25 Alternatively, if you want to show all columns that belong to students who are older than 25, you would do it like that: survey[survey$age &gt; 25,] ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 3 3 3 3 Maximilian 29 2021-09-09 Zeitung 4 1 ## 6 6 6 6 Andrea 26 2021-09-09 Online 3 4 ## 7 7 7 7 Fabienne 26 2021-09-09 TV 3 2 ## 8 8 8 8 Fabio 27 2021-09-09 Online 0 1 ## 10 10 10 10 Tim 26 2021-09-07 TV NA 2 ## 11 11 11 11 Alex 27 2021-09-09 Online NA 2 ## 12 12 12 12 Tobias 26 2021-09-07 Online 2 2 ## 14 14 14 14 Sabrina 27 2021-09-08 Online 1 2 ## 15 15 15 15 Valentin 29 2021-09-09 TV 1 5 ## 16 16 16 16 Tristan 26 2021-09-09 TV 2 5 ## 20 20 20 20 Florian 26 2021-09-09 Online 1 5 Of course, you can index more than one column: survey[survey$age &gt; 25,4:5] ## name age ## 3 Maximilian 29 ## 6 Andrea 26 ## 7 Fabienne 26 ## 8 Fabio 27 ## 10 Tim 26 ## 11 Alex 27 ## 12 Tobias 26 ## 14 Sabrina 27 ## 15 Valentin 29 ## 16 Tristan 26 ## 20 Florian 26 Finally, you can use multiple conditions: survey[survey$age &gt;= 25 &amp; survey$outlet_trust &lt; 3,] ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 3 3 3 3 Maximilian 29 2021-09-09 Zeitung 4 1 ## 7 7 7 7 Fabienne 26 2021-09-09 TV 3 2 ## 8 8 8 8 Fabio 27 2021-09-09 Online 0 1 ## 10 10 10 10 Tim 26 2021-09-07 TV NA 2 ## 11 11 11 11 Alex 27 2021-09-09 Online NA 2 ## 12 12 12 12 Tobias 26 2021-09-07 Online 2 2 ## 13 13 13 13 Michael 25 2021-09-09 Online 3 2 ## 14 14 14 14 Sabrina 27 2021-09-08 Online 1 2 3.7 Take-Aways Setting the working directory: tells R where your folder with the data is located on your drive, setwd(your_filepath) Import data: after setting the working directory, with read.csv() (comma-separated) or read.csv2() (semicolon-separated) Access variables: either by the access operator $ (dataframe&amp;variablename) or by the column index [,columnNo.] Access values: by indexing, i.e. using [ ] and row + column indices Conditions: you can select rows based on conditions, e.g.: greater &gt;, greater or equal &gt;=, equal ==, and not equal != 3.8 Additional tutorials You still have questions? The following online guides can help you with that: Import CSV Files into R Step-by-Step Guide Subsetting data Indexing into a data structure Now it’s your time to get into coding: Try Exercise 1. "],["exercise-1.html", "Exercise 1 Task 1 Task 2 Task 3 Task 4 Task 5 Task 6", " Exercise 1 After working through Exercise 1, you’ll… have assessed how well you know R and RStudio know what chapters and concepts you might want to repeat again have managed to apply the basic concepts of R to data Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What panels are part of RStudio? source console input packages, files &amp; plots 2. How do you activate R packages after you have installed them? import.packages() install.packages() package() library() 3. How do you create a vector in R with elements 1, 2, 3? cbind(1,2,3) cb(1,2,3) c(1,2,3) cmb(1,2,3) 4. Imagine you have a vector called ‘vector’ with 10 numeric elements. How do you retrieve the 8th element? vector[-2] vector[„-2”] vector[8] vector[„8”] 5. Imagine you have a vector called ‘hair’ with 5 elements: brown, black, red, blond, other. How do you retrieve the color ‘blond’? hair[4] hair[„4”] hair[blond] hair[„blond”] Task 2 Create a numeric vector with 8 values and assign the name age to the vector. First, display all elements of the vector. Then print only the 5th element. After that, display all elements except the 5th. Finally, display the elements at the positions 6 to 8. Task 3 Create a non-numeric, i.e. character, vector with 4 elements and assign the name eye_color to the vector. First, print all elements of this vector to the console. Then have only the value in the 2nd element displayed, then all values except the 2nd element. At the end, display the elements at the positions 2 to 4. Task 4 Get the “data_tutorial2.csv” from Moodle ( 4. Mai material folder ) and put it into the folder that you want to use as working directory. Set your working directory and load the data into R by saving it into a source object called data. Note: This time, it’s a csv that is actually separated by commas, not by semicolons. Task 5 Now, print only the age column to the console. Use the $ operator first. Then try to achieve the same result using the subsetting operators, i.e. []. Task 6 Print only the first 6 age numbers to the console. Use the $ operator first. Then try to achieve the same result using the subsetting operators, i.e. []. When you’re ready to look at the solutions, you can find them here: Solutions for Exercise 1. "],["tutorial-data-management-with-tidyverse.html", " 4 Tutorial: Data management with tidyverse 4.1 Why not stick with Base R? 4.2 Tidyverse packages 4.3 Tidy data 4.4 The pipe operator 4.5 Data transformation with dplyr 4.6 Take-Aways 4.7 Additional tutorials", " 4 Tutorial: Data management with tidyverse After working through Tutorial 4, you’ll… know the advantages of the tidyverse vs. Base R know about different formats of tabular data understand what packages are included in the tidyverse meta-package know how to do data modifications and transformations with dplyr 4.1 Why not stick with Base R? You might wonder why we’ve spent so much time exploring functions in Base R to now learn data management with tidyverse. After all, data management can also be done in Base R, can’t it? I personally recommend that all R beginners should work with the tidyverse as early as possible. There are three reasons supporting my argument: Ease of use: The tidyverse is very accessible for R “beginners”, i.e. its syntax is very easy to understand. It allows you to set goals (i.e. what you want to do with your data) and get you working on these goals very quickly. Definitely more quickly than in Base R! Standard for data management: A few years ago, the tidyverse has become the de facto standard for data management in R. It is a meta-package, which means that it is a collection of distinct packages that all follow the same design principles to make code reading and writing as simple as possible. For example, all functions are named after verbs that indicate exactly what they perform (e.g. filter or summarize). Beautiful graphs: With the tidyverse, all data management steps can be swiftly transferred into beautiful graphs. This is because the most popular graph package in R, ggplot2, is part of the tidyverse. Are you excited now? Then let’s get started! 4.2 Tidyverse packages The tidyverse comes with a great arsenal of topic-specific packages and their respective functions. It includes packages for: tibble: creating data structures like tibbles, which is an enhanced type of data frame readr, haven, readxl: reading data (e.g. readr for CSV, haven for SPSS, Stata and SAS, readxl for Excel) tidyr, dplyr: data transformation, modification, and summary statistics stringr, forcats, lubridate: create special, powerful object types (e.g. stringr for working with text objects, forcats for factors, lubridate for time data) purrr: programming with R ggplot2: graphing/charting The most frequently used packages of the tidyverse can be installed and activated in one go (less frequently used packages like haven still need to be installed and activated separately): install.packages(&quot;tidyverse&quot;) # install the package (only on the first time) library(tidyverse) # active the package 4.3 Tidy data Dataframes, which we learned about in [Types of objects], are tabular data. However, data can also have other formats, for example as nested, i.e. hierarchical, lists. In communication research, these other data formats are mainly used by social media and their respective APIs (perhaps you have heard of the “JSON” format before). In our course, however, we’ll focus on tabular data.The same data can be represented differently in tables. We perceive some of these representations as tidy, others as messy. While tidy data principles establish a standard for organizing data values inside a data frame and thus all tidy data look the same, every messy dataset is messy in its own way. Take a look at the table below. It shows a Starwars data set that comes pre-installed with the dplyr package. Do you feel the tabled data is messy? Why (not)? ## # A tibble: 10 × 3 ## name body_feature value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anakin Skywalker height 188 ## 2 Anakin Skywalker mass 84 ## 3 Chewbacca height 228 ## 4 Chewbacca mass 112 ## 5 Darth Vader height 202 ## 6 Darth Vader mass 136 ## 7 Jabba Desilijic Tiure height 175 ## 8 Jabba Desilijic Tiure mass 1358 ## 9 Leia Organa height 150 ## 10 Leia Organa mass 49 Overall, this data is messy. It comes with three messy problems: This body_feature column comprises information relating to both height and weight, i.e. both variables are stored in a single column. As a result, the value column is reliant on the body_feature column; we can’t tell the stored values apart by merely looking at the value column. We always need to check the body_feature column. Consequently, we have issues with vectorized functions (remember, in R, columns in data sets are vectors): We can’t, for example, use the mean() function on the value column to determine the average weight of the Star Wars characters since the height values are also stored there. What do you think of this table? Is it messy? ## # A tibble: 5 × 3 ## name height mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Anakin Skywalker 188 84 ## 2 Chewbacca 228 112 ## 3 Darth Vader 202 136 ## 4 Jabba Desilijic Tiure 175 1358 ## 5 Leia Organa 150 49 This table looks tidy! Tidy data is a standard way of mapping the meaning of a dataset to its structure. We determine whether a dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. We consider a table tidy when it follows the following golden rules: Columns: Every column is one variable. Rows: Every row is one observation. Cells: Every cell contains one single value. Image: The tidy data principle (Source: R for Data Science) In messy data sets, on the other hand… Column headers are values, not variable names. Multiple variables are stored in one column. Variables are stored in both rows and columns. Multiple types of observational units are stored in the same table. A single observational unit is stored in multiple tables. Why should you be concerned about tidy data organization? There are two major advantages: When you have a consistent data structure, it is easier to learn the respective tools that work well with this data structure. dplyr, ggplot2, and all the other tidyverse packages are designed for working with tidy data. Putting variables in columns makes R’s vectorized nature shine. The majority of built-in R-functions (like the mean() function) works with vectors of values. As a result, the tidy reorganization of data seems only natural for a good work flow in R. If you have been working mainly with survey data, then you will already be familiar with these basic rules, as data export from survey software usually follows these principles. However, “real-world” data from databases or social media often does not follow these principles. That’s why it’s sometimes true to say that 80% of data analysis is spent on cleaning and transforming data. 4.4 The pipe operator Truly, dplyr is my favorite tidyverse package (even more so than ggplot2, which we’ll cover later!). It allows you to perform powerful data transformations in just a few simple steps. To this end, dplyr relies on the pipe operator (%&gt;%).2 The %&gt;% operator allows functions to be applied sequentially to the same source object in a concise manner, so that step-by-step transformations can be applied to the data. Therefore, we always call the source object first and then add each transformation step separated by the %&gt;% operator. Let’s illustrate this concept with an example. We’ll use the Starwars data set that you are already familiar with. starwars_data %&gt;% # First, we define the source object, i.e. the data frame that we want to transform, followed by the pipe operator plot() # Second, we specify which function should be performed on the source object, here: scatterplot Now, that’s not very impressive. We could do the same in Base R like this: plot(starwars_data) However, dplyr gets really impressive when you chain functions sequentially. You can apply certain selection criteria to your data and plot it in one go. For example, we might exclude the variable name from our scatter plot, since it’s not a metric variable anyway. Also, we might want to look only at those Star Wars characters taller than 170 cm. Let’s try it in a single run! starwars_data %&gt;% # Define the source object select(height, mass) %&gt;% # Keep only the height and mass column filter(height &gt; 170) %&gt;% # Filter all observations that are taller than 170cm plot() # Plot! Now try to do the same in Base R: plot(starwars_data[starwars_data$height&gt;170,]$mass~starwars_data[starwars_data$height&gt;170,]$height, xlab=&quot;height&quot;, ylab=&quot;mass&quot;) The Base R code is longer, more nested, and not as readable as the code written in dplyr. And the more selection criteria and functions you need to implement, the worse it gets. For example, imagine you would also want to exclude Star Wars characters with a mass bigger than 1200kg. Peace of cake with dplyr: starwars_data %&gt;% select(height, mass) %&gt;% filter(height &gt; 170) %&gt;% filter(mass &lt; 1200) %&gt;% plot() 4.5 Data transformation with dplyr dplyrcomes with five main functions: select(): select variables column by column, i.e. pick columns / variables by their names filter(): filter observations row by row, i.e. pick observations by their values arrange(): sort / reorder data in ascending or descending order mutate(): calculate new variables or transform existing ones summarize(): summarize variables (e.g. mean, standard deviation, etc.), best combined with group_by() 4.5.1 select() Scientists will frequently provide you with large data sets including hundreds of variables (often even more!). The first problem in this scenario is narrowing down the variables you are truly interested in. select() helps you to easily choose a suitable subset of variables. In this selection process, the name of the data frame is the source object, followed by the pipe %&gt;% operator. The expression that selects the columns that you are interested in comes after that. Take the Star Wars data, for example. The original data set has 87 observations (Star Wars characters) and 14 columns / variables (traits of these characters, e.g., birth_year, gender, and species). Yes, 14 columns is not a lot and you could get an overview of this data without subsetting columns. Let’s take a look at the original data frame: library(dplyr) # load dplyr starwars_data &lt;- starwars # assign the pre-installed starwars data from dplyr to a source object / variable starwars_data # print the content of the data frame to the console ## # A tibble: 87 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… 172 77 blond fair blue 19 male mascu… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 4 Darth V… 202 136 none white yellow 41.9 male mascu… ## 5 Leia Or… 150 49 brown light brown 19 fema… femin… ## 6 Owen La… 178 120 brown, gr… light blue 52 male mascu… ## 7 Beru Wh… 165 75 brown light blue 47 fema… femin… ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… ## 9 Biggs D… 183 84 black light brown 24 male mascu… ## 10 Obi-Wan… 182 77 auburn, w… fair blue-gray 57 male mascu… ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; For the sake of practice, let’s say we only want to analyze the species, birth_year, mass, and height of these characters. To simplify data handling, we want to keep only the respective columns. starwars_data %&gt;% # define the source object select(name, species, birth_year, mass, height) # keep only the name, species, birth_year, mass and height column ## # A tibble: 87 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 R5-D4 Droid NA 32 97 ## 9 Biggs Darklighter Human 24 84 183 ## 10 Obi-Wan Kenobi Human 57 77 182 ## # … with 77 more rows At the moment you have only printed the transformed data to the console. However, most of the time we want to keep the transformed data ready for further calculations. In this case we should assign the transformed data into a new source object, which we can access later. starwars_short &lt;- starwars_data %&gt;% # assign a new source object and define the old source object select(name, species, birth_year, mass, height) # keep only the name, species, birth_year, mass and height column Let’s print the new source object, starwars_short, to the console. starwars_short ## # A tibble: 87 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 R5-D4 Droid NA 32 97 ## 9 Biggs Darklighter Human 24 84 183 ## 10 Obi-Wan Kenobi Human 57 77 182 ## # … with 77 more rows You can also delete columns by making a reverse selection with the - symbol. This means that you select all columns except the one whose name you specify. starwars_short %&gt;% select(-name) # keep all columns except the name column (i.e. delete name column) ## # A tibble: 87 × 4 ## species birth_year mass height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Human 19 77 172 ## 2 Droid 112 75 167 ## 3 Droid 33 32 96 ## 4 Human 41.9 136 202 ## 5 Human 19 49 150 ## 6 Human 52 120 178 ## 7 Human 47 75 165 ## 8 Droid NA 32 97 ## 9 Human 24 84 183 ## 10 Human 57 77 182 ## # … with 77 more rows You can delete more than one column in one go: starwars_short %&gt;% select(-c(name,species)) # keep all columns except the name &amp; species column (i.e. delete these columns) ## # A tibble: 87 × 3 ## birth_year mass height ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 19 77 172 ## 2 112 75 167 ## 3 33 32 96 ## 4 41.9 136 202 ## 5 19 49 150 ## 6 52 120 178 ## 7 47 75 165 ## 8 NA 32 97 ## 9 24 84 183 ## 10 57 77 182 ## # … with 77 more rows Tip for advanced users: You can select columns and rename them at the same time. starwars_short %&gt;% select(&quot;character&quot;=name, &quot;age&quot;=birth_year) # select columns that you want to keep &amp; rename them ## # A tibble: 87 × 2 ## character age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Luke Skywalker 19 ## 2 C-3PO 112 ## 3 R2-D2 33 ## 4 Darth Vader 41.9 ## 5 Leia Organa 19 ## 6 Owen Lars 52 ## 7 Beru Whitesun lars 47 ## 8 R5-D4 NA ## 9 Biggs Darklighter 24 ## 10 Obi-Wan Kenobi 57 ## # … with 77 more rows 4.5.2 filter() filter() divides observations into groups depending on their values. The name of the data frame is the source object, followed by the pipe %&gt;% operator. Then follow the expressions that filter the data. Let’s only select human Star Wars characters in our transformed data set starwars_short: starwars_short %&gt;% filter(species==&quot;Human&quot;) ## # A tibble: 35 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 Darth Vader Human 41.9 136 202 ## 3 Leia Organa Human 19 49 150 ## 4 Owen Lars Human 52 120 178 ## 5 Beru Whitesun lars Human 47 75 165 ## 6 Biggs Darklighter Human 24 84 183 ## 7 Obi-Wan Kenobi Human 57 77 182 ## 8 Anakin Skywalker Human 41.9 84 188 ## 9 Wilhuff Tarkin Human 64 NA 180 ## 10 Han Solo Human 29 80 180 ## # … with 25 more rows And now let’s only select Star Wars character who are younger than 24 or exactly 24 years old. starwars_short %&gt;% filter(birth_year&lt;=24) ## # A tibble: 7 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 Leia Organa Human 19 49 150 ## 3 Biggs Darklighter Human 24 84 183 ## 4 Wedge Antilles Human 21 77 170 ## 5 IG-88 Droid 15 140 200 ## 6 Wicket Systri Warrick Ewok 8 20 88 ## 7 Plo Koon Kel Dor 22 80 188 Chaining some functions, let’s look at Star Wars character who are a Droid and older than 24. starwars_short %&gt;% filter(species==&quot;Droid&quot; &amp; birth_year &gt; 24) # &amp; --&gt; filter all observations to which both logical statements apply ## # A tibble: 2 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 Alternatively, you can also write these filters like this: starwars_short %&gt;% filter(species==&quot;Droid&quot;) %&gt;% filter(birth_year &gt; 24) ## # A tibble: 2 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 Besides the &amp; operator, there are many more logical operators that you can choose from to optimize your filter choices. Here is an overview: Image: Logical, i.e. boolean, operators (Source: R for Data Science) Tip for advanced users 1: You can negate filters. This means that you keep all observations except the one that you have specified with the != operator (read != as: is not or is unequal to). For example, you can choose to include only non-human Star Wars characters. starwars_short %&gt;% filter(species!=&quot;Human&quot;) ## # A tibble: 48 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 R5-D4 Droid NA 32 97 ## 4 Chewbacca Wookiee 200 112 228 ## 5 Greedo Rodian 44 74 173 ## 6 Jabba Desilijic Tiure Hutt 600 1358 175 ## 7 Yoda Yoda&#39;s species 896 17 66 ## 8 IG-88 Droid 15 140 200 ## 9 Bossk Trandoshan 53 113 190 ## 10 Ackbar Mon Calamari 41 83 180 ## # … with 38 more rows Alternatively, you achieve the same goal by negating the entire function call. Negating the entire function call can be handy at times. starwars_short %&gt;% filter(!(species==&quot;Human&quot;)) ## # A tibble: 48 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 R5-D4 Droid NA 32 97 ## 4 Chewbacca Wookiee 200 112 228 ## 5 Greedo Rodian 44 74 173 ## 6 Jabba Desilijic Tiure Hutt 600 1358 175 ## 7 Yoda Yoda&#39;s species 896 17 66 ## 8 IG-88 Droid 15 140 200 ## 9 Bossk Trandoshan 53 113 190 ## 10 Ackbar Mon Calamari 41 83 180 ## # … with 38 more rows Tip for advanced users 2: You can filter for missing values (NAs) with the is.na() function. starwars_short %&gt;% filter(is.na(birth_year)) ## # A tibble: 44 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 R5-D4 Droid NA 32 97 ## 2 Jek Tono Porkins Human NA 110 180 ## 3 Arvel Crynyd Human NA NA NA ## 4 Nien Nunb Sullustan NA 68 160 ## 5 Nute Gunray Neimodian NA 90 191 ## 6 Roos Tarpals Gungan NA 82 224 ## 7 Rugor Nass Gungan NA NA 206 ## 8 Ric Olié &lt;NA&gt; NA NA 183 ## 9 Watto Toydarian NA NA 137 ## 10 Sebulba Dug NA 40 112 ## # … with 34 more rows And you can negate that filter to get rid of all observation that have missing values (NAs). starwars_short %&gt;% filter(!is.na(birth_year)) ## # A tibble: 43 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 Biggs Darklighter Human 24 84 183 ## 9 Obi-Wan Kenobi Human 57 77 182 ## 10 Anakin Skywalker Human 41.9 84 188 ## # … with 33 more rows Tip for advanced users 3: Watch out for the | operator (read: or). This one can be tricky to negate! For example, with this code you get all characters that are NEITHER human NOR older than 33 years. I.e. you get all non-human characters who are younger than 33 or exactly 33 years old. starwars_short %&gt;% filter(!((species == &quot;Human&quot;) | (birth_year &gt; 33))) ## # A tibble: 4 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 R2-D2 Droid 33 32 96 ## 2 IG-88 Droid 15 140 200 ## 3 Wicket Systri Warrick Ewok 8 20 88 ## 4 Plo Koon Kel Dor 22 80 188 But with this code, you’ll get all observations that are either non-human (regardless of their age) OR humans who are older than 33 years old. starwars_short %&gt;% filter((species != &quot;Human&quot;) | (birth_year &gt; 33)) ## # A tibble: 67 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 Darth Vader Human 41.9 136 202 ## 4 Owen Lars Human 52 120 178 ## 5 Beru Whitesun lars Human 47 75 165 ## 6 R5-D4 Droid NA 32 97 ## 7 Obi-Wan Kenobi Human 57 77 182 ## 8 Anakin Skywalker Human 41.9 84 188 ## 9 Wilhuff Tarkin Human 64 NA 180 ## 10 Chewbacca Wookiee 200 112 228 ## # … with 57 more rows 4.5.3 arrange() arrange() and filter() are like two brothers: both look similar, but they also differ in at least one essential aspect. Both functions change the rows of the data frame, but unlike filter(), arrange() does not select or delete rows, it only changes their order (either ascending or descending). By default, arrange() will sort in ascending order, i.e. from 1:100 (numeric vector) and from A:Z (character vector). arrange() must always be applied to at least one column that is to be sorted. starwars_short %&gt;% arrange(birth_year) ## # A tibble: 87 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Wicket Systri Warrick Ewok 8 20 88 ## 2 IG-88 Droid 15 140 200 ## 3 Luke Skywalker Human 19 77 172 ## 4 Leia Organa Human 19 49 150 ## 5 Wedge Antilles Human 21 77 170 ## 6 Plo Koon Kel Dor 22 80 188 ## 7 Biggs Darklighter Human 24 84 183 ## 8 Han Solo Human 29 80 180 ## 9 Lando Calrissian Human 31 79 177 ## 10 Boba Fett Human 31.5 78.2 183 ## # … with 77 more rows To get a descending order: starwars_short %&gt;% arrange(desc(birth_year)) ## # A tibble: 87 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Yoda Yoda&#39;s species 896 17 66 ## 2 Jabba Desilijic Tiure Hutt 600 1358 175 ## 3 Chewbacca Wookiee 200 112 228 ## 4 C-3PO Droid 112 75 167 ## 5 Dooku Human 102 80 193 ## 6 Qui-Gon Jinn Human 92 89 193 ## 7 Ki-Adi-Mundi Cerean 92 82 198 ## 8 Finis Valorum Human 91 NA 170 ## 9 Palpatine Human 82 75 170 ## 10 Cliegg Lars Human 82 NA 183 ## # … with 77 more rows If you specify more than one column, then subsequent columns are used to break ties. Also note that missing values are always displayed last: starwars_short %&gt;% arrange(species, birth_year) ## # A tibble: 87 × 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Ratts Tyerell Aleena NA 15 79 ## 2 Dexter Jettster Besalisk NA 102 198 ## 3 Ki-Adi-Mundi Cerean 92 82 198 ## 4 Mas Amedda Chagrian NA NA 196 ## 5 Zam Wesell Clawdite NA 55 168 ## 6 IG-88 Droid 15 140 200 ## 7 R2-D2 Droid 33 32 96 ## 8 C-3PO Droid 112 75 167 ## 9 R5-D4 Droid NA 32 97 ## 10 R4-P17 Droid NA NA 96 ## # … with 77 more rows 4.5.4 mutate() Often you want to add new columns to a data set, e.g. when you calculate new variables or when you want to store re-coded values of other variables. With mutate(), new columns will be added to the end of you data frame. For example, we can resize the height column to provide the body height in m instead of cm. Let’s call that variable m_height. We’ll assign our transformed data (with the newly created m_height column) back into our source object (starwars_short) to keep the changes for the future (and not just print it to the console). starwars_short &lt;- starwars_short %&gt;% # assigns your source object, i.e. data, back to itself to save changes mutate(m_height=height/100) # creates the new variable &quot;m_height&quot; and adds it to the end of the data frame starwars_short # print the data to your console to inspect the new column ## # A tibble: 87 × 6 ## name species birth_year mass height m_height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 ## 2 C-3PO Droid 112 75 167 1.67 ## 3 R2-D2 Droid 33 32 96 0.96 ## 4 Darth Vader Human 41.9 136 202 2.02 ## 5 Leia Organa Human 19 49 150 1.5 ## 6 Owen Lars Human 52 120 178 1.78 ## 7 Beru Whitesun lars Human 47 75 165 1.65 ## 8 R5-D4 Droid NA 32 97 0.97 ## 9 Biggs Darklighter Human 24 84 183 1.83 ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 ## # … with 77 more rows Let’s calculate the BMI of the Star Wars characters with the BMI formula and the newly created m_height variable. Save the changes to your data frame by assigning the source object back to itself. starwars_short &lt;- starwars_short %&gt;% # assigns your source object, i.e. data, back to itself to save changes mutate(BMI= mass/m_height^2) # creates the new variable &quot;BMI&quot; and adds it to the end of the data frame starwars_short # print the data to your console to inspect the new column ## # A tibble: 87 × 7 ## name species birth_year mass height m_height BMI ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 ## 2 C-3PO Droid 112 75 167 1.67 26.9 ## 3 R2-D2 Droid 33 32 96 0.96 34.7 ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 ## 5 Leia Organa Human 19 49 150 1.5 21.8 ## 6 Owen Lars Human 52 120 178 1.78 37.9 ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 ## 8 R5-D4 Droid NA 32 97 0.97 34.0 ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 ## # … with 77 more rows mutate() does not merely work with mathematical operators. You can also categorize numeric variables with the case_when function, which is part of the mutate() function. starwars_short &lt;- starwars_short %&gt;% mutate(age_cat= case_when( # &quot;cat&quot; is short for &quot;categorized&quot; birth_year &lt; 20 ~ &quot;very young&quot;, birth_year &lt; 40 ~ &quot;young&quot;, birth_year &lt; 70 ~ &quot;mid-aged&quot;, birth_year &lt;= 100 ~ &quot;old&quot;, birth_year &gt; 100 ~ &quot;very old&quot;) ) starwars_short ## # A tibble: 87 × 8 ## name species birth_year mass height m_height BMI age_cat ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 very young ## 2 C-3PO Droid 112 75 167 1.67 26.9 very old ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 mid-aged ## 5 Leia Organa Human 19 49 150 1.5 21.8 very young ## 6 Owen Lars Human 52 120 178 1.78 37.9 mid-aged ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 mid-aged ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 young ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 mid-aged ## # … with 77 more rows Finally, you can recode variables by using the recode() function, which is part of the mutate() function. Let’s be crazy and recode all droids as robots3 and save the result in a new variable called crazy_species! Please note that recode() has an unusual syntax because it follows the order of old_var = new_var instead of the usual order: new_var = old_var. Therefore, recode() is likely to be retired in the future (use case_when instead). starwars_short &lt;- starwars_short %&gt;% mutate(crazy_species=recode( # alternatively, you could also recode directly back into the species variable species, &quot;Droid&quot;=&quot;Robot&quot;)) starwars_short ## # A tibble: 87 × 9 ## name species birth_year mass height m_height BMI age_cat crazy_species ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… Human 19 77 172 1.72 26.0 very y… Human ## 2 C-3PO Droid 112 75 167 1.67 26.9 very o… Robot ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young Robot ## 4 Darth V… Human 41.9 136 202 2.02 33.3 mid-ag… Human ## 5 Leia Or… Human 19 49 150 1.5 21.8 very y… Human ## 6 Owen La… Human 52 120 178 1.78 37.9 mid-ag… Human ## 7 Beru Wh… Human 47 75 165 1.65 27.5 mid-ag… Human ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; Robot ## 9 Biggs D… Human 24 84 183 1.83 25.1 young Human ## 10 Obi-Wan… Human 57 77 182 1.82 23.2 mid-ag… Human ## # … with 77 more rows Tip for advanced users 1: There is a special case of recoding: Sometimes you will receive data (e.g. through an import from SPSS) in which missings are not marked as NA, but with -9 (or any other number). Unfortunately, you will have to tell R that these are missing values and should be set to NA. In this case, use the na_if() function, which is also part of the mutate() function. Luke Skywalker, the first observation in our data frame, is 172cm tall. For the sake of practice, let’s set all heights that are equal to 172cm to NA. This time, we won’t save this transformation for later use (by reassigning the source object back to itself) since this transformation does not make a lot of sense. starwars_short %&gt;% mutate(height= na_if(height, 172)) ## # A tibble: 87 × 9 ## name species birth_year mass height m_height BMI age_cat crazy_species ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… Human 19 77 NA 1.72 26.0 very y… Human ## 2 C-3PO Droid 112 75 167 1.67 26.9 very o… Robot ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young Robot ## 4 Darth V… Human 41.9 136 202 2.02 33.3 mid-ag… Human ## 5 Leia Or… Human 19 49 150 1.5 21.8 very y… Human ## 6 Owen La… Human 52 120 178 1.78 37.9 mid-ag… Human ## 7 Beru Wh… Human 47 75 165 1.65 27.5 mid-ag… Human ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; Robot ## 9 Biggs D… Human 24 84 183 1.83 25.1 young Human ## 10 Obi-Wan… Human 57 77 182 1.82 23.2 mid-ag… Human ## # … with 77 more rows 4.5.5 summarize() [+ group_by()] Instead of using summarize(), you could omit the American English and write summarise(). This function collapses a data frame into a single row that shows you summary statistics about your variables. Be careful not to overwrite your source object with the collapsed data frame, i.e. do not reassign the source object to itself when you use summarize() (unless you have a really good reason to do so). starwars_short %&gt;% summarize(mean_height = mean(height, na.rm=TRUE)) # collapses the data frame into one variable called &quot;mean_height&quot; ## # A tibble: 1 × 1 ## mean_height ## &lt;dbl&gt; ## 1 174. # na.rm = TRUE -&gt; removes the missing values prior to the computation of the summary We now know that the average Star Wars character is 174cm tall. But the summarize()function grows especially powerful when it is combined with `group_by to display summary statistics for groups. starwars_short %&gt;% group_by(species) %&gt;% # every unique species becomes its own group summarize(mean_height = mean(height, na.rm=TRUE), # collapses the data frame into one row with one variable called &quot;mean_height&quot;... count = n() # and a second variable that shows the group size (i.e. count) ) ## # A tibble: 38 × 3 ## species mean_height count ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Aleena 79 1 ## 2 Besalisk 198 1 ## 3 Cerean 198 1 ## 4 Chagrian 196 1 ## 5 Clawdite 168 1 ## 6 Droid 131. 6 ## 7 Dug 112 1 ## 8 Ewok 88 1 ## 9 Geonosian 183 1 ## 10 Gungan 209. 3 ## # … with 28 more rows We learn from the 6 droids in our data set that droids are small, 131cm on average. But Ewoks are even smaller (88cm on average). Pro tip: you can even group by two groups at the same time with the method group_by(x1, x2, .add=TRUE). Finally, we can also retrieve all relevant summary statistics of a classic box plot: starwars_short %&gt;% group_by(species) %&gt;% # every unique species becomes its own group summarize(MAX = max(height, na.rm = TRUE), UQ= quantile(height, 0.75, na.rm = TRUE), Med = median(height, na.rm = TRUE), M = mean(height, na.rm = TRUE), # SD = sd(height, na.rm = TRUE), # calculating the standard deviation is useless here because we often have only 1 observation per species LQ= quantile(height, 0.25, na.rm = TRUE), MIN = min(height, na.rm = TRUE), count = n() # shows the group_size (i.e. count) ) ## # A tibble: 38 × 8 ## species MAX UQ Med M LQ MIN count ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Aleena 79 79 79 79 79 79 1 ## 2 Besalisk 198 198 198 198 198 198 1 ## 3 Cerean 198 198 198 198 198 198 1 ## 4 Chagrian 196 196 196 196 196 196 1 ## 5 Clawdite 168 168 168 168 168 168 1 ## 6 Droid 200 167 97 131. 96 96 6 ## 7 Dug 112 112 112 112 112 112 1 ## 8 Ewok 88 88 88 88 88 88 1 ## 9 Geonosian 183 183 183 183 183 183 1 ## 10 Gungan 224 215 206 209. 201 196 3 ## # … with 28 more rows Tip for advances users 2: If you want to count the unique values of variables, then data %&gt;% group_by(a, b) %&gt;% summarize(n = n()) might not be the best solution (it’s a lot of code, isn’t it?). starwars_short %&gt;% group_by(species, age_cat) %&gt;% summarize(count = n()) ## # A tibble: 51 × 3 ## # Groups: species [38] ## species age_cat count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Aleena &lt;NA&gt; 1 ## 2 Besalisk &lt;NA&gt; 1 ## 3 Cerean old 1 ## 4 Chagrian &lt;NA&gt; 1 ## 5 Clawdite &lt;NA&gt; 1 ## 6 Droid very old 1 ## 7 Droid very young 1 ## 8 Droid young 1 ## 9 Droid &lt;NA&gt; 3 ## 10 Dug &lt;NA&gt; 1 ## # … with 41 more rows For more efficient code, you can use the count() function instead: data %&gt;% count(a, b). starwars_short %&gt;% count(species, age_cat) ## # A tibble: 51 × 3 ## species age_cat n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Aleena &lt;NA&gt; 1 ## 2 Besalisk &lt;NA&gt; 1 ## 3 Cerean old 1 ## 4 Chagrian &lt;NA&gt; 1 ## 5 Clawdite &lt;NA&gt; 1 ## 6 Droid very old 1 ## 7 Droid very young 1 ## 8 Droid young 1 ## 9 Droid &lt;NA&gt; 3 ## 10 Dug &lt;NA&gt; 1 ## # … with 41 more rows 4.5.6 Chaining functions in a pipe All of the dplyr functions can be chained in one single pipe. Using the original starwars_data, we’ll only analyze Star Wars characters who are older than 25 years (filter()), calculate the BMI (mutate()), group them by their species (group_by()) and summarize the average BMI (summarize()). We’ll display the final result in an ascending order (arrange()). starwars_data %&gt;% mutate(BMI = mass/(height/100)^2) %&gt;% filter(birth_year&gt;25) %&gt;% group_by(species) %&gt;% summarize(mean_BMI = mean(BMI, na.rm = TRUE)) %&gt;% arrange(mean_BMI) ## # A tibble: 14 × 2 ## species mean_BMI ## &lt;chr&gt; &lt;dbl&gt; ## 1 Gungan 17.2 ## 2 Twi&#39;lek 17.4 ## 3 Mirialan 18.8 ## 4 Cerean 20.9 ## 5 Wookiee 21.5 ## 6 Rodian 24.7 ## 7 Human 25.3 ## 8 Mon Calamari 25.6 ## 9 Zabrak 26.1 ## 10 Droid 30.8 ## 11 Trandoshan 31.3 ## 12 Yoda&#39;s species 39.0 ## 13 Hutt 443. ## 14 &lt;NA&gt; NaN 4.6 Take-Aways Tidy data: is a tabular in which each column represents one single variable, each row represents a single observation and each cell contains only one single value Pipe operator: %&gt;% is used to chain functions and apply them to a source object. We call these chains of functions pipes dplyr functions: there are five main dplyr functions that you should know of: select, filter, arrange, mutate, and summarize [+ group_by]. 4.7 Additional tutorials You still have questions? The following tutorials &amp; papers can help you with that: Computational Methods in der politischen Kommunikationsforschung by J. Unkel, Tutorial 7, 9 &amp; 10 R for Data Science, Chapter 12 R for Data Science, Chapter 5.1.3 and the following YaRrr! The Pirate’s Guide to R by N.D.Phillips, Tutorial 10.4 The tidyverse style guide Data wrangling with dplyr &amp; tidyr Cheat Sheet Now let’s see what you’ve learned so far: Exercise 2: Test your knowledge. To be precise, the pipe operator was introduced to R with the package magrittr, not with dplyr. Nowadays, the %&gt;% operator can be used outside the tidyverse package if magrittr is installed and loaded: library(magrittr).↩︎ Please, don’t hate me for this.↩︎ "],["exercise-2-test-your-knowledge.html", "Exercise 2: Test your knowledge Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7", " Exercise 2: Test your knowledge After working through Exercise 2, you’ll… have assessed how well you know dplyr know what dplyr functions and concepts you might want to repeat again have managed to apply the dplyr concepts to data Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What are the main characteristics of tidy data? Every cell contains values. Every cell contains a variable. Every observation is a column. Every observation is a row. 2. What are dplyr functions? summary() describe() mutate() manage() 3. How can you sort the eye_color of Star Wars characters from Z to A? starwars_data %&gt;% arrange(desc(eye_color)) starwars_data %&gt;% arrange(eye_color) starwars_data %&gt;% select(arrange(eye_color)) starwars_data %&gt;% select(eye_color) %&gt;% arrange(desc(eye_color)) 4. Imagine you want to recode the height of the these characters. You want to have three categories from small and medium to tall. What is a valid approach? starwars_data %&gt;% mutate(height = case_when(height&lt;=150~\"small\",height&lt;=190~\"medium\",height&gt;190~\"tall\")) starwars_data %&gt;% mutate(height = case_when(height&lt;=150~small,height&lt;=190~medium,height&gt;190~tall)) starwars_data %&gt;% recode(height = case_when(height&lt;=150~\"small\",height&lt;=190~\"medium\",height&gt;190~\"tall\")) starwars_data %&gt;% recode(height = case_when(height&lt;=150~small,height&lt;=190~medium,height&gt;190~tall)) 5. Imagine you want to provide a systematic overview over all hair colors and what species wear these hair colors frequently (not accounting for the skewed sampling of species)? What is a valid approach? starwars_data %&gt;% group_by(hair_color) %&gt;% group_by(species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) starwars_data %&gt;% group_by(hair_color, species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) starwars_data %&gt;% group_by(hair_color &amp; species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) starwars_data %&gt;% group_by(hair_color + species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) Task 2 It’s you turn now. Load the starwars data like this: library(dplyr) # to activate the dplyr package starwars_data &lt;- starwars # to assign the pre-installed starwars data set (dplyr) into a source object in our environment How many humans are contained in the starwars data overall? (Hint: use summarize(count = n()) or count()) Task 3 How many humans are contained in starwars by gender? Task 4 What is the most common eye_color among Star Wars characters? (Hint: use arrange()) Task 5 What is the average mass of Star Wars characters that are not human and have yellow eyes? (Hint: remove all NAs) Task 6 Compare the mean, median, and standard deviation of mass for all humans and droids. (Hint: remove all NAs) Task 7 Create a new variable in which you store the mass in gram (gr_mass). Add it to the data frame. Test whether your solution works by printing your data to the console, but only show the name, species, mass, and your new variable gr_mass. When you’re ready to look at the solutions, you can find them here: Solutions for Exercise 2. Are you ready for some beautiful graphs? Then check out the next Tutorial: Data visualization with ggplot. "],["tutorial-data-visualization-with-ggplot.html", " 5 Tutorial: Data visualization with ggplot 5.1 Why not stick with Base R? 5.2 Components of a ggplot graph 5.3 Installing &amp; activating ggplot 5.4 Building your first plot 5.5 Other common plot types 5.6 Take Aways 5.7 Additional tutorials", " 5 Tutorial: Data visualization with ggplot After working through Tutorial 5, you’ll… know what each graphical component of a ggplot graph contributes to the final visualization understand the grammer of graphics (or simply: the ggplot2 syntax) to combine graphical components know how to make your own data visualizations using ggplot2 5.1 Why not stick with Base R? The ggplot2 package, i.e. the data visualization package of tidyverse, has become the R package for data visualization. While Base R can be used to visualize data, the ggplot2 package makes data visualization so much easier that I recommend starting with ggplot2 right away and skipping data visualization in Base R altogether. The gg in ggplot2 stands for grammar of graphics, which means that we can describe each component of a graph layer by layer and component by component. You only have to provide ggplot() with a source object (i.e. data) and specify what variables it should map to the aesthetical attributes (color, shape, size) of certain geometric objects (points, lines, bars) – and ggplot will take care of the rest! The inventor of ggplot2, Hadley Wickham, describes the benefits of ggplot2 like this: “In order to unlock the full power of ggplot2, you’ll need to master the underlying grammar. By understanding the grammar, and how its components fit together, you can create a wider range of visualizations, combine multiple sources of data, and customise to your heart’s content… The grammar makes it easier for you to iteratively update a plot, changing a single feature at a time. The grammar is also useful because it suggests the high-level aspects of a plot that can be changed, giving you a framework to think about graphics, and hopefully shortening the distance from mind to paper. It also encourages the use of graphics customised to a particular problem, rather than relying on specific chart types.” (Wickham et al., 2021, no page; bold words inserted) Just as dplyr simplifies data manipulation, ggplot2 simplifies data visualization. In addition, ggplot2 and dplyr work hand in hand: You can prepare your data selection and manipulation with dplyr and pipe it directly into ggplot to turn your transformed data into a beautiful graph. With only a few lines of code, you can produce graphs like this one: This is the code. Right now, it might still look a bit overwhelming to you, but once you’ve understood the grammar of graphics, it really is a just a small jigsaw puzzle. Moreover, you don’t usually start with graphs that are this complicated, but with basic scatter or bar plots. library(ggplot2) plot &lt;- starwars_data %&gt;% filter(species == &quot;Human&quot; | species == &quot;Droid&quot;) %&gt;% ggplot(aes(x = height, y = mass, size = birth_year, fill = species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + ggrepel::geom_text_repel(aes(label = name), size = 2.3) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) To visit the official documentation of ggplot2: - type ?ggplot2 in your console - visit the ggplot documentation - visit the ggplot homepage of the tidyverse 5.2 Components of a ggplot graph As mentioned before, the main idea behind ggplot is to generate a statistical plot by combining layers that represent geometric objects (e.g. points and lines). By linking data to the aesthetic features of these geometric objects (e.g. colors, size, transparency), the aesthetic properties of the geometric objects may be controlled. In the words of Wickham: “A graphic maps the data to the aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars).” Wickham et al., 2021, no page; bold words inserted Image: The logic of adding layer by layer in ggplot (Source: R @ Ewah 2020): The necessary components of a ggplot graph are: Source object / data: The data that you would like to visualize. Geometries geom_: Geom options allow you to specify what geometric objects will represent the data (i.e. points, bars, lines, and many more). Aesthetics aes(): Aesthetics allows you to map variables to the x- and y-axis and to the aesthetics of those geometric objects (i.e. position, color, size, shape, linetype, and transparency). The complementary, but not necessary components of a ggplot graph are: Scales scale_: Scale options allow you to fine-tune the mapping from the variables to the aesthetics. You can fine-tune axis limits, tick breaks, grid lines, or any other axis/geometric object transformations that depend on the range of a specific scale. Statistical transformations stat_: Allows you to produce statistical summaries of the data for visualization (i.e. means and standard deviations, fitted curves, and many more). Coordinate system coord_: Allows you to change the appearance of your coordinate system (i.e. flip the coordinates to turn horizontal bar chart into a vertical one). Position: to adjust overlapping objects, e.g. jittering, stacking or dodging. Facets facet_: Allows you to divide your plot into multiple subplots. Visual themes theme(): Allows you to specify the visual basics of a plot, such as background, default typeface, sizes, and colors. Axis labels labs(): Allows you to change the plot’s main title and the axis labels. 5.3 Installing &amp; activating ggplot You can always activate ggplot2 by activating the meta-package tidyverse: library(tidyverse) If for some reason you do not want to activate the whole tidyverse, you should install ggplot2 and activate this package separately: install.packages(&quot;ggplot2&quot;) # install the package (only on the first time) library(ggplot2) # active the package 5.4 Building your first plot In the next sections, you will create your very first plot – layer by layer. We will look at some of the most important components that you will regularly add to graphs and you will learn how to make use of them. 5.4.1 Data Obviously, you need data to perform data visualization. Therefore, our first step is to load the starwars data, but let’s keep only humans and droids for now. To this end, assign your transformed data to a new data frame called human_droid_data. human_droid_data &lt;- dplyr::starwars %&gt;% filter(species == &quot;Human&quot; | species == &quot;Droid&quot;) The function ggplot() can only create a plot if we explicitly tell the function what data to use, so this graphical component is necessary. Using our dplyr skills, let’s use the human_droid_data as our source object and apply the ggplot() function to it by using a pipe (i.e. %&gt;%). human_droid_data %&gt;% ggplot() The ggplot() function creates a blank canvas (i.e. first layer). We now have to draw on it. 5.4.2 Aesthetics To draw on this blank canvas, we must at least tell the ggplot() function which variables to assign to the x- and y-axis by using the aes() function. Thus, the Aesthetics graph component is also necessary in every single plot. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) The aes() function allows you to specify the following arguments (and many more, as you will learn over time): x: the variable that should be mapped to the x axis y: the variable that should be mapped to the y size: the variable that should be used for determining the size of a geometric object fill: the variable that should be used for filling a geometric object with a specific color color: the variable that should be used for outlining a geometric object with a specific color 5.4.3 Geometrics Finally, we can turn to the last necessary component of any ggplot graph: the geometric objects that fill your canvas. The choice of these geometric objects determines what kind of chart you create. The geom_ component of the ggplot() function allows you to create the following chart types (and many more, as you will learn over time): geom_bar(): to create a bar chart geom_histogram: to create a histogram geom_line(): to create a line graph geom_point(): to create a scatter or bubble plot geom_boxplot(): to create a box plot Now let’s add the data points (x,y) with geom_point() to our canvas to make it a scatter plot: human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() That’s a scatter plot for sure! And you only needed three necessary components to create it: data (i.e. a source object), aesthetics aes(), and geometric objects geom_. 5.4.4 Scales A scale is a mapping from data to the final values that computers can use to actually show the aesthetics. In this sense, a scale regulates the aesthetic mapping of variables to aesthetics. Providing a scale_ is not necessary to create a graph, but it allows you to fine-tune aesthetic mappings to customize your graph. scale_ is very powerful and over time, you will learn about a lot of things that you can customize with it. For now, we will only focus on a few of these. We will use scale_ to : change the limits and ticks of the x and y axis change how a third variable (besides x and y) is mapped to the aesthetics of our geometric object First, we will use scale_ to modify the x and the y axis by providing the graph with new axis limits. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + # modify the y axis limits scale_x_continuous(limits = c(90, 210)) # modify the x axis limits Second, we’ll add more ticks to make the graph better readable. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + # choose where the ticks of the y axis appear scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) # choose where the ticks of the x axis appear Until now, we have used scale_ to transform only the axes. But we can also use it to change the mapping of variables to geometric objects. To demonstrate this, we now add another variable to our graph, namely the age (birth_year) of the humanoid and droid Star Wars characters. Let’s map age to our data points (i.e. geom_point()) so that larger bubbles reflect older age. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + # map birth_year (age) to the size of the following geometric objects geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) Personally, I feel like these bubbles could use a little bit of rescaling to make age differences stand out more. In addition, you could get a nicer title for the size legend than “birth_year”. Let’s try that. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) # sets the bubbles&#39; size in a range between 1 and 11 and renames the respective legend title to &quot;age&quot; Perfect! The legend spells “age” and age differences seem a bit more obvious now. Unfortunately, some data points are now overlapping. I think this is a good time to introduce you to the differences between using scale_ and adding aesthetics to the geom_ objects directly. While the former allows you to change the mapping from variables to the aesthetics of geometric objects, the latter one allows you to provide a constant. This means that the aesthetic mapping does not depend on the values of a variable, but is set to a single default value. To demonstrate this and fix the overlap of our bubbles, we change the transparency value of the bubbles so that they become transparent. Note that all the values given are constants, which means that they do not depend on a third variable like birth_year. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + # shape = 21 is creating bubbles that have a border (i.e. outline), fill = &quot;black&quot; fills the bubble with black ink, alpha = 0.25 to make the bubbles` black ink 25% transparent and color = &quot;black&quot; to make the border (i.e. outline) pitch black scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) This looks way more readable. I think we are ready to move on to Themes. 5.4.5 Themes Just like scale_, theme_is an optional ggplot component, i.e. not necessary. Themes are visually appealing presets for charts, e.g., they influence whether grid lines are visible or whether certain color palettes are applied to the data. By using themes you can make your graphs more beautiful and give them a consistent style without any effort, which is especially useful for longer texts like theses. To familiarize yourself with the various options, take a look at this overview of all ggplot2 themes. If you don’t like grid lines, for example, theme_classic() might be to your taste: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_classic() Personally, I really enjoy the theme_bw() (black-and-white theme). So let’s apply it to our graph: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() 5.4.6 Labs Again, labs() is not a necessary, but an optional component of your graph. Using the labs() function allows you to set a main title for your plot and to change the labels of the x and y axis. Let’s try it: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) Now that we’ve added a main title, it becomes clear that we can’t really distinguish the data points that represent humans from those that represent droids. 5.4.7 Facets Faceting divides plot into tiny subplots, which display different subsets of your data. Facets are an effective way to explore your data because they allow you to rapidly detect divergent patterns in these subsets. Of course, faceting is optional, i.e. not necessary. You don’t need faceting if you don’t want to compare different groups within your data. The two approaches to faceting are: facet_wrap(): uses the levels of one (or more) variable(s) to create groups + panels for each group; useful if you have a single categorical variable with many levels facet_grid(): produces a matrix of panels defined by two variables which form the rows and columns Image: The logic of faceting (Source: Wickham et al., 2021): Let’s use the facet_wrap() function to create two subplots for our two different levels of the species variable: Droid and Human. You can provide two arguments to facet_wrap(): ~, followed by the grouping variable nrow: the number of rows in which panels should be placed human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species, nrow=2) # using ~grouping_variable and nrow = 2 shows the two panels on top of each other Great, finally we can distinguish the data points representing humans from those representing droids! On the left side, however, the panel with the humans looks a bit empty. Maybe we should put them next to each other. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) # nrow=1 is the default, so you don´t have to call it explicitly I like that! 5.4.8 Saving graphs I think it’s time that we save this plot. To finish of this “masterpiece” (and make it less triste), let’s add some final colors before saving. We’ll fill our bubbles with colorful ink based on the species variable, so we need to add fill=species to the aes() and remove the default black ink provided in the geom_point() function. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year, fill=species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) Congratulations! You have just managed to recreate the plot from the beginning of this tutorial! The only thing we haven’t covered yet is the labeling of all data points, because for that you’d need the ggrepel package to not mess up the labels - and that’s not part of ggplot2. So let’s skip that and save our graph. To use your graph in another document, e.g. a theses written in Word, you’ll have to export the plot first. Therefore, you must assign your plot to a new object and call the ggsave() function on that object. The plot will be saved to your working directory and formatted according to the file extension you specified (for example: .jpeg or .png). plot &lt;- human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year, fill=species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) ggsave(filename = &quot;mass_vs_height.jpeg&quot;, plot) 5.5 Other common plot types I can’t give an overview of all possible types of plots, but I can at least touch a bit on how other common types of geom_ behave. 5.5.1 bar plots Bar plots are very common. They are either (1) used to display the frequency with which a certain factor level of a categorical variable occurs or (2) to display relationships between a categorical variable and a metric variable. So let’s create a quick bar plot using the sex variable (categorical, three factor levels) and get an overview on how many human and droidic Star Wars characters are male, female, or do not have a sex. human_droid_data %&gt;% ggplot(aes(x = sex)) + # We only have to specify the variable that we want to get the count for (i.e. number of observations) geom_bar() Next, let’s look at the relationship between sex and the height (metric) variable. We will produce a bar plot that displays the mean height of each group: human_droid_data %&gt;% ggplot(aes(x = sex, y=height)) + # Now we need to specify the variable that we want to summarize with mean statistics geom_bar(stat = &quot;summary&quot;, fun.y = &quot;mean&quot;) # apply the summary statistic of y (mean) to the geom_bars Maybe we want to sort the bars according to their mean. Let’s reorder the factor levels manually. human_droid_data %&gt;% mutate(sex = factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;, &quot;none&quot;))) %&gt;% ggplot(aes(x = sex, y=height)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) And if we would like to have a horizontal bar plot, we can use the coord_ component of ggplot() to flip the coordinates. human_droid_data %&gt;% mutate(sex = factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;, &quot;none&quot;))) %&gt;% ggplot(aes(x = sex, y=height)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) + coord_flip() 5.5.2 box plots Box plots are a great option to summarize metric variables (by groups). They provide you with the Five-number-summary: the sample minimum (smallest observation) – lower whisker the lower quartile – lower end of the box the median (the middle value) – thick black line the upper quartile – upper end of the box the sample maximum (largest observation) – upper whisker Let’s create box plots of the height for human and droidic Star Wars characters who are male, female, or do not have a sex. human_droid_data %&gt;% ggplot(aes(x = sex, y=height)) + geom_boxplot() 5.6 Take Aways graph creation: ggplot() mapping variables to aesthetics: aes(x, y, color, fill, size, etc.) chart type: geom_bar(), geom_line(), geom_point(), geom_boxplot() (for example) titles: labs() axis limits/ticks: scale_x_continuous(), scale_y_continuous() mapping variables to geom size: scale_size() themes: theme_classic(), theme_light(), theme_bw() (for example) faceting: facet_wrap() or facet_grid save images: ggsave() 5.7 Additional tutorials You still have questions? The following tutorials &amp; papers can help you with that: Chang, W. R (2021). R Graphics Codebook. Practical Recipes for Visualizing Data. Link Wickham, H., Navarro, D., &amp; Pedersen, T. L. (2021). ggplot2: elegant graphics for data analysis. Online, work-in-progress version of the 3rd edition. Link Hehman, E., &amp; Xie, S. Y. (2021). Doing Better Data Visualization. Advances in Methods and Practices in Psychological Science. DOI: 10.1177/25152459211045334 Link R Codebook by J.D. Long and P. Teetor, Tutorial 10 Now let’s see what you’ve learned so far: Exercise 3: Test your knowledge. "],["exercise-3-test-your-knowledge.html", "Exercise 3: Test your knowledge Task 1 Task 2 Task 3", " Exercise 3: Test your knowledge After working through Exercise 3, you’ll… be able to see a graph and recreate it with ggplot2 be able to see a problem and customize a plot with ggplot2 to solve it Task 1 The data set glbwarm comes pre-installed with the processR package that we will be working with soon. The data comprises 815 US individuals (417 females, 398 males) who agreed to engage in online questionnaires. They roughly represent the population of the United States. Let’s install / activate the processR package first and assign the glbwarm data to a source object. # installing/loading the package: if(!require(processR)) { install.packages(&quot;processR&quot;); require(processR) } #load / install+load processR data &lt;- processR::glbwarm If you can’t install processR (it can be a bit buggy with newer versions of R) then just download the glbwarm.csv from Moodle and don’t use the code above (material folders). Load the data into R by following our tutorial: Import data from your working directory, but remember the difference between read_csv2 (simcolon-separated) and read_csv (comma-separated) and use the appropriate command. Familiarize yourself with the data set (Hint: use the help() / ? function or visit this Google page of the help function) and then try to reproduce this plot with dplyr and ggplot2. (Hint: You can hide the legend by adding theme(legend.position = \"none\") to your plot.) You will need to do some data management with dplyr to create the correct labels for your ggplot2 graph. Are you stuck and don’t know where to start? Watch this video to help you get started: Click here. Task 2 Now, try to reproduce this graph. (Hint: You will need to recode the ideology variable in a way that higher values represent stronger attitudes, independent of partisanship.) Task 3 Can you make a chart that breaks down the relationship between age, negative emotions about climate change, and ideological extremity for the different sexes AND parties? Hint: You can play around with which variables that you want to use as x, y, and as color / size variables. But if you want to have a recommendation to get you started: x = age, y = negative emotions about climate change, size = ideological extremity. But other combinations are valid, too! For instance, you could decide to map x = ideological extremity and size = age. When you’re ready to look at the solutions, you can find them here: Solutions for Exercise 3. "],["exercise-4-test-your-knowledge.html", "Exercise 4: Test your knowledge Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8", " Exercise 4: Test your knowledge After working through Exercise 4, you’ll… have repeated the most important functions of dplyr and ggplot2 This exercise was created because some students asked to get some more practice. You don’t have to work through it, but it can certainly help you with the graded assignment. In this exercise, we will work with the mtcars data that comes pre-installed with dplyr. library(tidyverse) data &lt;- mtcars # To make the data somewhat more interesting, let&#39;s set a few values to missing values: data$wt &lt;- na_if(data$wt, 4.070) data$mpg &lt;- na_if(data$mpg, 22.8) Let’s first get to know this data. We can get some information about the variables that are included in the dataset by using Rs help() function: help(mtcars) Image: Output of the help(mtcars) function: We get to know that this is a data set was created from the 1974 Motor Trend US magazine, and that it comprises fuel consumption (mpg) and 10 other aspects (cyl, wt) of automobile design and performance for 32 different cars. Task 1 Check the data set for missing values (NAs) and delete all observations that have missing values. Task 2 Let’s transform the weight wt of the cars. Currently, it’s given as Weight in 1000 lbs. I guess you are not used to lbs, so try to mutate wt to represent Weight in 1000 kg. 1000 lbs = 453.59 kg, so we will need to divide by 2.20. Similarly, I think that you are not very familiar with the unit Miles per gallon of the mpg variable. Let’s transform it into Kilometer per liter. 1 m/g = 0.425144 km/l, so again divide by 2.20. Task 3 Now we want to group the weight of the cars in three categories: light, medium, heavy. But how to define light, medium, and heavy cars, i.e., at what kg should you put the threshold? A reasonable approach is to use quantiles (see Tutorial: summarize() [+ group_by()]). Quantiles divide data. For example, the 75% quantile states that exactly 75% of the data values are equal or below the quantile value. The rest of the values are equal or above it. Use the lower quantile (0.25) and the upper quantile (0.75) to estimate two values that divide the weight of the cars in three groups. What are these values? Task 4 Use the values from Task 3 to create a new variable wt_cat that divides the cars in three groups: light, medium, and heavy cars. Task 5 How many light, medium, and heavy cars are part of the data? Task 6 Now sort this count of the car weight classes from highest to lowest. Task 7 Make a scatter plot to indicate how many km per liter (mpg) a car can drive depending on its weight (wt). Facet the plot by weight class (wt_cat). Try to hide the plot legend (you have learned that in another exercise). Task 8 Recreate the diagram from Task 7, but exclude all cars that weigh between 1.4613636 and 1.5636364 *1000kg from it. When you’re ready to look at the solutions, you can find them here: Solutions for Exercise 4. We have officially finished our chapters on data management and visualization! Next, we can start with digital text manipulation! "],["tutorial-text-manipulation-with-stringr.html", " 6 Tutorial: Text manipulation with stringr 6.1 What’s stringr? 6.2 Working with strings 6.3 Working with string patterns 6.4 Working with regular expressions 6.5 Take-Aways 6.6 Additional tutorials", " 6 Tutorial: Text manipulation with stringr After working through Tutorial 6, you’ll… understand the concept of string patterns and regular expressions know how to search for string patterns 6.1 What’s stringr? The stringr package is another package of the tidyverse family, i.e., it comes pre-installed with the tidyverse. The package offers a neat set of functions that makes working with strings really simple for beginners. Therefore, stringr is a good place to start getting into text data management. A string is a data type that is used to represent text rather than numbers. 6.2 Working with strings First, let’s create a vector that contains some strings and print the vector to the console! fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;, &quot;rhubarb&quot;) fruits ## [1] &quot;banana&quot; &quot;apple&quot; &quot;pear&quot; &quot;strawberry&quot; &quot;raspberry&quot; ## [6] &quot;kiwi&quot; &quot;rhubarb&quot; First, we want to know how long each of these strings is, i.e., how many characters the elements of the fruits vector contain. We will use the str_length() function. str_length(fruits) ## [1] 6 5 4 10 9 4 7 That was easy. Next, we want to join multiple strings into a single string. We will use the str_c function. str_c(fruits, collapse = &quot; and &quot;) ## [1] &quot;banana and apple and pear and strawberry and raspberry and kiwi and rhubarb&quot; # If collapse is not NULL, it will be inserted between elements of the result, here: and You can also change the order of the str_c function: str_c(&quot;My favourite fruit is: &quot;, fruits, collapse = NULL) ## [1] &quot;My favourite fruit is: banana&quot; &quot;My favourite fruit is: apple&quot; ## [3] &quot;My favourite fruit is: pear&quot; &quot;My favourite fruit is: strawberry&quot; ## [5] &quot;My favourite fruit is: raspberry&quot; &quot;My favourite fruit is: kiwi&quot; ## [7] &quot;My favourite fruit is: rhubarb&quot; Let’s say, we want to extract substrings from a character vector. For example, we only want to keep the second to fourth letter of each string. We can use the str_sub function to achieve that. str_sub(fruits, start = 2, end = 4) ## [1] &quot;ana&quot; &quot;ppl&quot; &quot;ear&quot; &quot;tra&quot; &quot;asp&quot; &quot;iwi&quot; &quot;hub&quot; 6.3 Working with string patterns Often we want to search for certain string patterns in a text document. String patterns are character sequences (for instance, letter, numbers, or special characters). Let’s assume for a second that we have misspelled one of our fruits (banana, we have switched the na letters to an). misspelled_fruits &lt;- c(&quot;baanan&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;, &quot;rhubarb&quot;) It would be really great if we could search for the string pattern “an” and replace it with the string pattern “na” automatically, wouldn’t it? Well, stringroffers some functions to do just that. For example, str_detect() tells you if there’s any match to the pattern. str_detect(misspelled_fruits, &quot;an&quot;) ## [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE Now we know that there is one word that contain letters that match the “an” pattern. We have a misspelling! What is that word? str_subset() extracts the matching strings, so that we can find out. str_subset(misspelled_fruits, &quot;an&quot;) ## [1] &quot;baanan&quot; Of course, it’s banana. But how many times has “an” been misspelled in banana? Just once? Let’s find out with str_count(, which counts the number of patterns in each string. str_count(misspelled_fruits, &quot;an&quot;) ## [1] 2 0 0 0 0 0 0 Two times! Let’s fix that with the str_replace function. misspelled_fruits &lt;- str_replace(misspelled_fruits, &quot;anan&quot;, &quot;nana&quot;) misspelled_fruits ## [1] &quot;banana&quot; &quot;apple&quot; &quot;pear&quot; &quot;strawberry&quot; &quot;raspberry&quot; ## [6] &quot;kiwi&quot; &quot;rhubarb&quot; Perfect! We have fixed our misspelled fruits. However, keep in mind that pattern correction can mess up your string pretty badly if you are not cautious. Therefore, you should always explore your strings very thoroughly before replacing any string patterns. For example, let’s see what our pattern detection will uncover if our misspelled fruits would contain an additional orange, which has been spelled correctly: misspelled_fruits &lt;- c(&quot;baanan&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;, &quot;rhubarb&quot;, &quot;orange&quot;) str_detect(misspelled_fruits, &quot;an&quot;) ## [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE Now, str_detect() matches two words with the “an” pattern, but the latter is not a misspelling! So always be careful! As a final lesson, you can also split a string into multiple strings based on certain string patterns using the str_split()function: cs_fruits &lt;- c(&quot;banana, apple, pear, strawberry, raspberry, kiwi, rhubarb, orange&quot;) str_split(cs_fruits, &quot;,&quot;) ## [[1]] ## [1] &quot;banana&quot; &quot; apple&quot; &quot; pear&quot; &quot; strawberry&quot; &quot; raspberry&quot; ## [6] &quot; kiwi&quot; &quot; rhubarb&quot; &quot; orange&quot; 6.4 Working with regular expressions Often, we want to match more complicated string patterns than a simple “an”. For example, we might wish to detect all strings in our text document that do not start with “RT”, because “RT” at the beginning of a string implies a retweet rather than an original tweet when analyzing Twitter data. Arguably, we are often not really interested in analyzing retweets (but sometimes we are, it depends on the research question). To search and match complex string patterns, we need regular expressions. Regular expressions (short: regex) are a concise language for describing patterns of text. Regex should not be taken literally, but have a non-literal meaning. Let’s keep working with our (non-misspelled) fruits vector to display what regex can do. First, let’s look for all strings that start with the letter b using the ^ (start of string) regex. str_detect(fruits, &quot;^b&quot;) # ^ stands for &quot;start of string&quot;, i.e. we are matching for strings that start with the letter b ## [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE That’s on point because only our first entry, banana, starts with b and str_detect() matched that correctly! With a similar approach, you can find all fruits that end with the letter b: str_detect(fruits, &quot;b$&quot;) # $ stands for &quot;end of string&quot;, i.e. we are matching for strings that end with the letter b ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE Again, we have a perfect match of the only fruit that ends with the letter b: rhubarb. Please, note the difference to not matching these two regexes (^ and $), but the simple string pattern “b”: str_detect(fruits, &quot;b&quot;) # matches all strings that contain the letter b at any place ## [1] TRUE FALSE FALSE TRUE TRUE FALSE TRUE This has matched banana, strawberry, raspberry, and rhubarb, because all of these fruits contain a letter b at some place. Finally, you should also not confuse “^b” with “[^b]”, because [^] stands for “anything but” in regex language. fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;, &quot;rhubarb&quot;, &quot;b&quot;, &quot;bbb&quot;) str_detect(fruits, &quot;[^b]&quot;) # matches all strings that contain any letters different from b(s) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE Next, let’s say that it doesn’t really matter to use whether our fruits contains the letter b or the letter e at any position of the string. There is a very powerful regex to match this string pattern: the “match any one of” operator []. fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;, &quot;rhubarb&quot;) str_detect(fruits, &quot;[be]&quot;) # matches all strings that contain either the letter b or the letter e ## [1] TRUE TRUE TRUE TRUE TRUE FALSE TRUE Our regex has managed to match all strings that contain either the letter b or the letter e, which leaves only kiwi to be FALSE. Next, we can match all fruits that contain letters that range between s to w in the ABC. We will need to use the range operator [-]: str_detect(fruits, &quot;[s-w]&quot;) # matches all strings that contain either the letter s, t, u, v, or, w ## [1] FALSE FALSE FALSE TRUE TRUE TRUE TRUE Again, we have a perfect match! The str_detect has successfully matched “strawberry”, “raspberry”, “kiwi”, and “rhubarb”. Next, we might want to match all fruits that contain more than one r, i.e., we want to match strawberry and raspberry, but not pear. This is where the ? operator (zero or one) * operator (zero or more), the + operator (one or more), and the {n} operator (exactly n) come in handy. str_detect(fruits, &quot;r?&quot;) # matches all strings that contain zero or one rs ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE str_detect(fruits, &quot;r*&quot;) # matches all strings that contain zero or more rs ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE str_detect(fruits, &quot;r+&quot;) # matches all strings that contain one or more rs --&gt; this grabs pear as well, not there yet! ## [1] FALSE FALSE TRUE TRUE TRUE FALSE TRUE str_detect(fruits, &quot;r{2}&quot;) # matches all strings that contain exactly two rs --&gt; this grabs only the berries, yeah! ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE This looks great! We now know the most important regular expressions! If you feel like you need even more advanced regular expressions, you can look them up in this awesome stringr cheat sheet. 6.5 Take-Aways String patterns &amp; RegEx: String patterns are sequences of characters; regular expressions are a type of string patterna used to match or detect other string patterns in texts. Important regular expressions: The best overview of all regex options can be found on the stringr cheat sheet. 6.6 Additional tutorials You still have questions? The following tutorials, books, &amp; tools may help you: Stringr Cheat Sheet Regexr - Learn, build, and test RegEx Text as Data by V. Hase, Tutorial 9 "],["exercise-5-test-your-knowledge.html", "Exercise 5: Test your knowledge Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7", " Exercise 5: Test your knowledge First, load the library stringr and create a new car vector: library(stringr) cars &lt;- c(&quot;VW&quot;, &quot;Mercedes-Benz&quot;, &quot;BMW&quot;, &quot;Audi&quot;, &quot;Opel&quot;, &quot;Skoda&quot;, &quot;Ford&quot;, &quot;Seat&quot;, &quot;Cupra&quot;, &quot;Hyundai&quot;, &quot;Renault&quot;, &quot;Fiat&quot;, &quot;Toyota&quot;, &quot;Peugeot&quot;, &quot;Volvo&quot;) Task 1 You have a list of the most popular cars in Europe (according to car buyers in 2021). Marketers believe that the secret to product sales is a catchy name, often between 3 and 4 letters. Is this true? Try to find out how long the brands’ names are. Task 2 Some brands cheat: They actually have very long names and use acronyms to make their brand more catchy. Let’s expose them! Replace all acronyms with the real brand name, i.e., VW = Volkswagen and BMW = Bayerische Motoren Werke. Task 3 There is proof that some sounds are cacophonic, such as voiced fricatives (/f/,/v/,/s/,/z/,/h/), while others are regarded euphonic, such as vowels (/a/, /e/) and liquids (/l/, /r/) (see here). Marketers sometimes recommend not to use cacophonic sounds in brand names. Find all car brands that use cacophonic sounds despite these warnings. Task 4 How many cacophonic sounds are used in these brand names, i.e., are there any brands that use (at least) two cacophonic sounds? Task 5 Which brand names begin with a cacophonic sound? Task 6 Which brand names end with a cacophonic sound? Task 7 Let’s correct those cacophonic brand names by replacing all cacophonic sounds with a liquid sound. Let’s replace all inner cacophonic sounds with /l/ and all cacophonic sounds that start a brand name with /R/. When you are ready to look at the solutions, see here: Solutions for Exercise 5. "],["tidy-text-analysis.html", " 7 Tidy text analysis 7.1 What is tidy text? 7.2 Preprocessing real-world data 7.3 (Relative) word frequencies 7.4 Topic modeling 7.5 Sentiment analysis 7.6 Take-Aways 7.7 Additional tutorials", " 7 Tidy text analysis After working through Tutorial 7, you’ll… understand the concept of tidy text know how to combine tidy text management approaches with regular expressions be able to produce first analyses, e.g., word frequencies 7.1 What is tidy text? Since you’ve already learnt what tidy data is (see Tidy data), you can make an educated guess about the meaning of “tidy text”! In contrast to the ways text is commonly stored in existing text analysis approaches (e.g., as strings or document-term matrices), the tidy text format is a table with one single token per row (token = the unit of analysis). A token is a meaningful unit of text, i.e., a word, a sentence, or a paragraph that we are interested in using for further analysis. Splitting text into tokens is what we call the process of tokenization. Julia Silge and David Robinson’s tidytext package makes tokenizing into the tidy text format simple! Moreover, the tidytext package builds upon tidyverse and ggplot2, which makes it easy to use for you, since you already know these packages (see Tutorial: Data management with tidyverse and Tutorial: Data visualization with ggplot for a recap). That’s why we’ll focus on the tidytext in this breakout session. This tutorial is based on Julia Silge and David Robinson’s open-access book “Text Mining in R. A Tidy Approach” and a lot of the following code was actually written by Julia Silge. If you want to dig deeper into tidy text analysis, you should check the book out. Both authors have also created an informative flowchart of the tidy text analysis workflow: Image: Tidy Text Analysis Workflow Before we start, install and load the tidytext package, the tidyverse package &amp; the lubridate package. lubridate will convert dates to a standard format, making it easier to create graphs using time data. Since tweets come with a date column, it’s always useful to have the lubridate package loaded. # installing/loading the package: if(!require(tidytext)) { install.packages(&quot;tidytext&quot;); require(tidytext) } #load / install+load tidytext # installing/loading lubridate: if(!require(lubridate)) { install.packages(&quot;lubridate&quot;); require(lubridate) } #load / install+load lubridate library(lubridate) library(tidytext) library(tidyverse) 7.2 Preprocessing real-world data Let’s investigate what the tidytext package is capable of and apply it to some real-world data. I’ve provided you with a data file that contains data collected with the Twitter API about the Russian aggression against Ukraine on Moodle. Let’s import the data into RStudio and inspect it using the View() function. data &lt;- read.csv(&quot;ukraine_tweets2.csv&quot;, encoding = &quot;UTF-8&quot;) # Use UTF-8 encoding to keep the smileys Sys.setenv(TZ=&quot;UTC&quot;) # Twitter gives a UTC timestamp, so we&#39;ll need to set our own environment to UTC, too data &lt;- data %&gt;% mutate(time = lubridate::ymd_hms(time)) %&gt;% # turn the data column into a proper format with the lubridate package (this helps to create time series graphs!) tibble() View(data) This data set contains a lot of information! We get to know who wrote the tweet (user and the unique user.id) and where this person lives (user.location). But most importantly, we can find the text of the tweet in the column full_text. First of all, let’s reduce some of these columns that we don’t need for our text analysis. data_short &lt;- data %&gt;% select(time, user, full_text) # keeps time, user, and full_text View(data_short) Now, to get an overview, let’s create a visualization of the user’s who posted at least 18 tweets. data_short %&gt;% group_by(user) %&gt;% summarize(n = n()) %&gt;% filter(n &gt; 18) %&gt;% ggplot(aes(x = user, y = n)) + stat_summary(geom = &quot;bar&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, vjust = 0.00, hjust = 0.00)) + labs(title = &quot;Users who posted the most tweets about the Russian\\naggression against Ukraine&quot;, x = &quot;User name&quot;, y = &quot;Number of Tweets&quot;) Now that we have had our overview, we will start with a process called text normalization. Text normalization is the endeavor to minimize the variability of a text by bringing it closer to a specified “standard” by reducing the quantity of divergent data that the computer has to deal with. In plain English, this means that our goal is to reduce the amount of text to a small set of words that are especially suitable to explain similarities and differences between tweets and, more interestingly, between the accounts that have published these tweets. Overall, text normalization boosts efficiency. Some techniques of text normalization will be covered in the next few sections, for example, tokenization, stop word removal, stemming/lemmatization and pruning. You could add additional steps, e.g. spelling corrections to reduce misspellings that increase word variance, but we won’t cover all text normalization steps in this tutorial. 7.2.1 Tokenization I think we are ready to tokenize the data with the amazing unnest_tokens() function of the tidytext package. Tokenization is the process of breaking text into words (and punctuation marks). Since scientists are almost never interested in the punctuation marks and will delete them from the data set later anyway, unnest_tokens() comes with the nice bonus of setting all text to lowercase and deleting the punctuation marks directly, leaving only words (and numbers) as tokens. We will use the highly specialized token = \"tweets\" option of the tidytext package (by Mullen 2016), which preserves hashtags and mentions of users with the @ sign from the direct punctuation removal process performed by unnest tokens(). Another common practice is to remove all tokens that contain numbers. Depending on the research question, it can also be helpful to remove URLs. For illustrative purposes, we will do both. But keep in mind that removing all URLs will take the option from you to compare what accounts have shared certain URLs more frequently than others. remove_reg &lt;- &quot;&amp;amp;|&amp;lt;|&amp;gt;&quot; # &amp;amp; = &amp; in HTML, &amp;lt; = &lt; in HTML, &amp;gt; = &gt; in HTML data_tknzd &lt;- data_short %&gt;% mutate(tweet = row_number()) %&gt;% # creates a new column that is called &quot;tweet&quot;, which contains a unique id for each individual tweet based on its row number filter(!str_detect(full_text, &quot;^RT&quot;)) %&gt;% # removes all retweets, i.e., tweets that are not unique but a reposts mutate(text = str_remove_all(full_text, remove_reg)) %&gt;% # remove special HTML characters (&amp;, &lt;, &gt;) unnest_tokens(word, full_text, token = &quot;tweets&quot;) %&gt;% # using `to_lower = TRUE` with `token = &#39;tweets&#39;` can break URLs, so be sure that you don&#39;t need them filter(!str_detect(word, &quot;^[:digit:]+$&quot;)) %&gt;% # remove all words that are numbers, e.g. &quot;2020&quot; filter(!str_detect(word, &quot;^http&quot;)) %&gt;% # remove all words that are a URL, e.g., (https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf) filter(!str_detect(word, &quot;^(\\\\+)+$&quot;)) %&gt;% # remove all words that only consist of plus signs (e.g., &quot;+++&quot;) filter(!str_detect(word, &quot;^(\\\\+)+(.)+$&quot;)) %&gt;% # remove all words that only consist of plus signs followed by any other characters (e.g., &quot;+++eil+++&quot;) filter(!str_detect(word, &quot;^(\\\\-)+$&quot;)) %&gt;% # remove all words that only consist of plus signs (e.g., &quot;---&quot;) filter(!str_detect(word, &quot;^(\\\\-)+(.)+$&quot;)) %&gt;% # remove all words that only consist of minus signs followed by any other characters (e.g., &quot;---eil---&quot;) filter(!str_detect(word, &quot;^(.)+(\\\\+)+$&quot;)) # remove all words that start with some kind of word followed by plus signs (e.g., &quot;news+++&quot;) head(data_tknzd) ## # A tibble: 6 × 5 ## time user tweet text word ## &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… @von… ## 2 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… @euc… ## 3 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… rubb… ## 4 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… your ## 5 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… words ## 6 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… and As you can see, unnest tokens() in combination with the token = \"tweets\" option has kept all mentions as tokens. Add %&gt;% filter(!str_detect(word, \"^@\")) at the end of the above presented code if you want to remove these, too. Now, let’s check the total number of unique words in our tweet data: paste(&quot;We are investigating &quot;, dim(data_tknzd)[1], &quot; non-unique features / words and &quot;, length(unique(data_tknzd$word)), &quot; unique features / words.&quot;) ## [1] &quot;We are investigating 250299 non-unique features / words and 23579 unique features / words.&quot; Remember: unnest tokens() comes with a few handy preprocessing features, i.e., it already turns the text into a standardized format for further analysis: Tweet ids from which each word originated are kept in the data frame (column: tweet). All punctuation has already been taken care of, i.e., dots, questions marks, etc. have been removed. All uppercase letters have been taken care of, i.e., words have been transformed to lowercase.4 Practice: Tokenization Now it’s your turn. I’ve added another data set to Moodle that deals with the shitstorm about the German influencer Fynn Kliemann on Twitter. You can read more about the scandal that started on the 6th of May 2022 here if you feel that you need more information. Load the Kliemann data into RStudio. Use the tutorial code to set the encoding. After loading the Kliemann data keep only the time, user, and full_text column. Next, try to tokenize the data. As an extra, delete all tokens that are mentions of other twitter users (i.e., that start with an @-smybol). For solutions see: Solutions for Exercise 6 7.2.2 Stop word removal Next, we should get rid of stop words. Stop words are a group of words that are regularly used in a language. In English, stop words such as “the,” “is,” and “and” would qualify. Because stop words are so common, they don’t really tell us anything about the content of a tweet and what differentiates this tweet from other tweets. The tidytext package comes with a pre-installed stop word data set. Let’s save that data set to a source object called stop_word_data. This way, we can use it later. stop_word_data &lt;- tidytext::stop_words head(stop_word_data, 20) # prints the first 20 stop words to the console ## # A tibble: 20 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 a SMART ## 2 a&#39;s SMART ## 3 able SMART ## 4 about SMART ## 5 above SMART ## 6 according SMART ## 7 accordingly SMART ## 8 across SMART ## 9 actually SMART ## 10 after SMART ## 11 afterwards SMART ## 12 again SMART ## 13 against SMART ## 14 ain&#39;t SMART ## 15 all SMART ## 16 allow SMART ## 17 allows SMART ## 18 almost SMART ## 19 alone SMART ## 20 along SMART Now, let’s use these stop words to remove all tokens that are stop words from our tweet data. data_tknzd &lt;- data_tknzd %&gt;% filter(!word %in% stop_word_data$word) %&gt;% # removes all of our tokens that are stop words filter(!word %in% str_remove_all(stop_word_data$word, &quot;&#39;&quot;)) # first removes all &#39; from the stop words (e.g., ain&#39;t -&gt; aint) and then removes all of our tokens that resemble these stop words without punctuation Practice: Stop word removal Now it’s your turn. The Kliemann data is in German, so you can’t use the tidytext stop word list, which is meant for English text only. So install and load the ‘stopwords’ package that allows you to create a dataframe that contains German stop words by using this command: stop_word_german &lt;- data.frame(word = stopwords::stopwords(\"de\"), stringsAsFactors = FALSE). Create your German stop word list and use it to remove stop words from your tokens. For solutions see: Solutions for Exercise 6 7.2.3 Lemmatizing &amp; stemming Minimizing words to their basic form (lemmatizing) or root (stemming) is a typical strategy of reducing the number of words in a text. Stemming: A stem is the root form of a word without any suffixes. A stemming algorithm (= stemmer) removes the suffixes and returns the stem of the word. Example1: vengeance –&gt; vengeanc, or, if you use a very aggressive stemmer, vengeance –&gt; veng || Example2: legions –&gt; legion, or, if you use a very aggressive stemmer, legions –&gt; leg (this one is problematic!) || Example3: murdered –&gt; murder Lemmatization: A lemma is a lexicon headword or, i.e., the dictionary-matched basic form of a word (as opposed to a stem created by eliminating or changing suffixes). Because a lemmatizing algorithm (= lemmatizer) performs dictionary matching, lemmatization is more computationally demanding than stemming. vengeance –&gt; vengeance || Example2: legions –&gt; legion || Example3: murdered –&gt; murdered (this one is problematic!) Most of the time, stemmers will make a lot of mistakes (e.g., legions –&gt; leg) and lemmatizers will make fewer mistakes. However, lemmatizers summarize fewer words (murdered –&gt; murdered) and therefore reduce the word count less efficiently than stemmers. Which technique of text normalization is preferred is always determined by the research question and the available data. For the ease of teaching, we will use stemming on our tweet data. However, you could also use lemmatization with the spacyr package. We’ll use Porter’s (1980) stemming algorthm, which is the most extensively used stemmer for the English language. Porter made the stemmer open-source, and you may use it with R using the SnowballC package (Bouchet-Valat 2020). # installing/loading SnowballC: if(!require(SnowballC)) { install.packages(&quot;SnowballC&quot;); require(SnowballC) } #load / install+load SnowballC data_tknzd &lt;- data_tknzd %&gt;% mutate(word = wordStem(word)) head(data_tknzd) ## # A tibble: 6 × 5 ## time user tweet text word ## &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… @von… ## 2 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… @euc… ## 3 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… rubb… ## 4 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… word ## 5 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… fell… ## 6 2022-02-25 18:10:59 peekay14 1 @vonderleyen @EU_Commission rubbish!… eu Let’s check how many unique words we have in our tweet data after all of our preprocessing efforts: paste(&quot;We are investigating &quot;, dim(data_tknzd)[1], &quot; non-unique features / words and &quot;, length(unique(data_tknzd$word)), &quot; unique features / words.&quot;) ## [1] &quot;We are investigating 120056 non-unique features / words and 18792 unique features / words.&quot; Practice: Lemmatizing &amp; stemming Please stem the Kliemann data with the PorterStemmer. Since we are working with German data, you’ll have to add the option language = \"german\" to the wordStem() function. For solutions see: Solutions for Exercise 6 7.2.4 Pruning Finally, words that appear in practically every tweet (i.e., promiscuous words) or words that that appear only once or twice in the entire data set (i.e., rare words) are not very helpful for investigating the “typical” word use of certain Twitter accounts. Since these words do not contribute to capturing similarities and differences in texts, it is a good idea to remove them. This process of (relative) pruning is quickly achieved with the quanteda package. Since I don’t want to teach you how to use quanteda just yet, I have developed a useful short cut function that enables you to use relative pruning without having to learn quanteda functions. First, install/load the quanteda package and initialize my prune-function by running this code: # Install / load the quanteda package if(!require(quanteda)) { install.packages(&quot;quanteda&quot;); require(quanteda) } #load / install+load quanteda prune &lt;- function(data, tweet_column, word_column, text_column, user_column, minDocFreq, maxDocFreq) { suppressWarnings(suppressMessages(library(&quot;quanteda&quot;, quietly = T))) # Grab the variable names for later use tweet_name &lt;- substitute(tweet_column) word_name &lt;- substitute(word_column) tweet_name_chr &lt;- as.character(substitute(tweet_column)) word_name_chr &lt;- as.character(substitute(word_column)) # turn the tidy data frame into a document-feature matrix: dfm &lt;- data %&gt;% select({{tweet_column}}, {{text_column}}, {{word_column}}, {{user_column}}) %&gt;% count({{tweet_column}}, {{word_column}}, sort = TRUE) %&gt;% cast_dfm({{tweet_column}}, {{word_column}}, n) # perform relative pruning: dfm &lt;- dfm_trim(dfm, min_docfreq = minDocFreq, # remove all words that occur in less than minDocFreq% of all tweets max_docfreq = maxDocFreq, # remove all words that occur in more than maxDocFreq% of all tweets docfreq_type = &quot;prop&quot;, # use probabilities verbose = FALSE) # don&#39;t tell us what you have achieved with relative pruning # turn the document-feature-matrix back into a tidy data frame: data2 &lt;- tidy(dfm) %&gt;% rename({{word_name}} := term) %&gt;% # rename columns so that their names from data_tknzd and data_tknzd2 match rename({{tweet_name}} := document) %&gt;% select(-count) %&gt;% # remove the count column mutate({{tweet_name}} := as.integer({{tweet_name}})) # delete the words that quanteda suggested from the original data data &lt;- right_join(data, data2, by = c(word_name_chr,tweet_name_chr)) %&gt;% # keep only the shorter data set without frequent/rare words, but fill in all other columns like user and date distinct({{tweet_name}},{{word_name}}, .keep_all= TRUE) # remove duplicate rows that have been created during the merging process return(data) } Now that you have my prune() function running, you can use it to prune your data. The prune function takes the following arguments: data: the name of your data set (here: data_tknzd), tweet_column: the name of the column that contains the tweet ids (here: tweet) word_column: the name of the column that contains the words/tokens (here: word), the name of the column that contains the full text of the tweets (here: text) text_column: the name of the column that contains the tweets’ full text (here: text) user_column: the name of the column that contains the user names (here: user) minDocFreq: the minimum values of a word’s occurrence in tweets, below which super rare words will be removed (e.g., occurs in less than 0.01% of all tweets, we choose here: 0.001) maxDocFreq: the maximum values of a word’s occurrence in tweets, above which super promiscuous words will be removed (e.g., occurs in more than 95% of all tweets, we choose here: 0.95) # Install / load the quanteda package for topic modeling data_tknzd &lt;- prune(data_tknzd,tweet,word,text,user, 0.001, 0.95) Finally, now that we have removed very rare and very frequent words, let’s check the number of our unique words again: paste(&quot;We are investigating &quot;, dim(data_tknzd)[1], &quot; non-unique features / words and &quot;, length(unique(data_tknzd$word)), &quot; unique features / words.&quot;) ## [1] &quot;We are investigating 83851 non-unique features / words and 1727 unique features / words.&quot; Great, we have finished the text normalization process! I think we are ready to take a sneak peek at our most common words! What are the 10 most commonly used words in our tweet data? I’m excited! data_tknzd %&gt;% count(word, sort = TRUE) %&gt;% slice_head(n=10) ## # A tibble: 10 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 ukrain 7555 ## 2 russia 2115 ## 3 putin 1478 ## 4 #ukrain 1364 ## 5 russian 1241 ## 6 peopl 1114 ## 7 war 1092 ## 8 nato 829 ## 9 invas 692 ## 10 countri 656 Evaluation: Shortly after Russia’s invasion of Ukraine began, Twitter conversations centered on the countries Ukraine, Russia, Putin, the people, the war, NATO, and the invasion. Practice: Pruning Please, try the prune function for yourself. Prune the Kliemann data and remove 1) words that occur in less than 0.03% of all tweets and 2) words that occur in more than 95% of all tweets. Beware: Pruning is a demanding task, therefore your machine might need a while to complete the computations. Wait until the red stop sign in your console vanishes. For solutions see: Solutions for Exercise 6 7.3 (Relative) word frequencies Now that we have finished our text normalization process, let’s get to know our data. Let’s first look at the absolute word frequencies of the 5 most active Twitter accounts in our data set to get to know them better. For this, we need to group our data by the most active Twitter accounts and count how many times each user used each word. Why is it intresting to know what words are used by what Twitter account? Word usage reveals a lot about the agenda and thought processes of communciators. Words have power, especially in times of conflict. Words, for example, might help define who is seen as the attacker and who is seen as the defender. It matters whether I call something an “attack” or a “war”, since an aggression is a unilateral act of invasion, while a war is a reciprocal relationship that has two parties involved. data_tknzd %&gt;% group_by(user) %&gt;% summarize(n = n()) %&gt;% arrange(desc(n)) %&gt;% filter(n &gt; 102) ## # A tibble: 19 × 2 ## user n ## &lt;chr&gt; &lt;int&gt; ## 1 DaveClubMember1 570 ## 2 common_sense54 553 ## 3 EdCutter1 444 ## 4 Seyahsm2 324 ## 5 BaloIreen 193 ## 6 AlinaPoltavets1 188 ## 7 dhart2001 167 ## 8 LateNighter5 160 ## 9 Alexandera000 156 ## 10 _Matheuu_ 153 ## 11 TicheyPamela 150 ## 12 tinfoilted1 146 ## 13 NilAndNull 132 ## 14 Vadim56691447 123 ## 15 Julli_a_ 119 ## 16 Bojagora 104 ## 17 inversedotcom 104 ## 18 Chris__Iverson 103 ## 19 Intrepid_2011 103 Evaluation: The five most active Twitter accounts are DaveClubMember1, common_sense54, EdCutter1, Seyahsm2, and BaloIreen. Let’s also add the less active LateNighter5 for the purpose of practice. frequency &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) # sorts the word, i.e., the most common words are displayed first frequency ## # A tibble: 139 × 3 ## user word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 DaveClubMember1 #prayforukrain 30 ## 2 DaveClubMember1 #russiavsukrain 30 ## 3 DaveClubMember1 #saveukrain 30 ## 4 DaveClubMember1 #stopputin 30 ## 5 DaveClubMember1 bio 30 ## 6 DaveClubMember1 discord 30 ## 7 DaveClubMember1 horribl 30 ## 8 DaveClubMember1 imag 30 ## 9 DaveClubMember1 join 30 ## 10 DaveClubMember1 peopl 30 ## # … with 129 more rows Evaluation: This is insightful. As we can see, the PorterStemmer treats ukranian and ukrain, american and america, as well as russian and russia as separate words. Since the part of speech (adjective or noun) is not particularly relevant for our analyses, we should merge the terms to focus on more meaningful TopWords. In addition, treating #ukrain and ukrain as separate words might not be hlpful. data_tknzd$word &lt;- str_replace_all(data_tknzd$word, c(&quot;russian&quot; = &quot;russia&quot;, &quot;ukrainian&quot; = &quot;ukrain&quot;, &quot;american&quot; = &quot;america&quot;, &quot;#ukrain&quot; = &quot;ukrain&quot;, &quot;#russia&quot; = &quot;russia&quot;)) frequency &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) # sorts the word, i.e., the most common words are displayed first frequency ## # A tibble: 135 × 3 ## user word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Seyahsm2 america 32 ## 2 DaveClubMember1 #prayforukrain 30 ## 3 DaveClubMember1 #saveukrain 30 ## 4 DaveClubMember1 #stopputin 30 ## 5 DaveClubMember1 bio 30 ## 6 DaveClubMember1 discord 30 ## 7 DaveClubMember1 horribl 30 ## 8 DaveClubMember1 imag 30 ## 9 DaveClubMember1 join 30 ## 10 DaveClubMember1 peopl 30 ## # … with 125 more rows Next, we would like now the relative word frequencies. Some twitter users might have posted a lot, while others have written little, but used some meaningful words (e.g. “aggression”) excessively. We want to know the share of these meaningful words as compared to the absolute number of words posted by the respective user. frequency &lt;- frequency %&gt;% left_join(data_tknzd %&gt;% count(user, name = &quot;total&quot;)) %&gt;% # total = how many words has that particular user used in total? mutate(freq = ((n/total)*100)) # freq = relative frequency of the respective word compared to the total number of words that the user has used frequency ## # A tibble: 135 × 5 ## user word n total freq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Seyahsm2 america 32 324 9.88 ## 2 DaveClubMember1 #prayforukrain 30 570 5.26 ## 3 DaveClubMember1 #saveukrain 30 570 5.26 ## 4 DaveClubMember1 #stopputin 30 570 5.26 ## 5 DaveClubMember1 bio 30 570 5.26 ## 6 DaveClubMember1 discord 30 570 5.26 ## 7 DaveClubMember1 horribl 30 570 5.26 ## 8 DaveClubMember1 imag 30 570 5.26 ## 9 DaveClubMember1 join 30 570 5.26 ## 10 DaveClubMember1 peopl 30 570 5.26 ## # … with 125 more rows Let’s make the user-specific word lists and their similarities / differences a little easier to interpret: frequency %&gt;% arrange(desc(n)) %&gt;% group_by(user) %&gt;% slice_head(n=10) %&gt;% arrange(desc(n)) %&gt;% select(user, word) %&gt;% summarize(terms = list(word)) %&gt;% mutate(terms = map(terms, paste, collapse = &quot;, &quot;)) %&gt;% unnest(cols = c(terms)) %&gt;% group_by(user) ## # A tibble: 6 × 2 ## # Groups: user [6] ## user terms ## &lt;chr&gt; &lt;chr&gt; ## 1 BaloIreen ukrain, russia, ban, directli, putin, sanction, shelter, sky,… ## 2 common_sense54 alli, biden, complet, cultiv, defens, democrat, explan, invas… ## 3 DaveClubMember1 #prayforukrain, #saveukrain, #stopputin, bio, discord, horrib… ## 4 EdCutter1 administr, afghan, amp, bail, biden, border, cashless, citi, … ## 5 LateNighter5 ukrain, bank, busi, call, classic, coup, democrat, die, fund,… ## 6 Seyahsm2 america, action, bold, border, hope, increas, invad, militari… You can even turn these user-specific word lists into a word cloud, if you like these kind of visualizations: # First install the ggwordcloud package: if(!require(ggwordcloud)) { install.packages(&quot;ggwordcloud&quot;); require(ggwordcloud) } #load / install+load ggwordcloud # Then store your word lists into a source object wordcloud_data &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) # Create the word cloud: wordcloud_data %&gt;% ggplot(aes(label = word, size = n, color = user)) + geom_text_wordcloud_area(show.legend = TRUE) + scale_size_area(max_size = 7) + scale_color_manual(values = c(&quot;#9FC6C0&quot;,&quot;#89BFC1&quot;,&quot;#67A9B6&quot;,&quot;#5495AD&quot;,&quot;#377099&quot;,&quot;#08333F&quot;)) + theme_bw() + guides(size = FALSE) Or this word cloud: wordcloud_data %&gt;% ggplot(aes(label = word, size = n, color = user)) + geom_text_wordcloud_area(show.legend = FALSE) + scale_size_area(max_size = 7) + scale_color_manual(values = c(&quot;#9FC6C0&quot;,&quot;#89BFC1&quot;,&quot;#67A9B6&quot;,&quot;#5495AD&quot;,&quot;#377099&quot;,&quot;#08333F&quot;)) + theme_bw() + facet_wrap(~user) Evaluation: Most of the Twitter accounts seem to be U.S. American. However, they focus on different issues. (1) BaloIreen seems to demand for economic actions against Russia, (2) common_sense54 seems to focus on a Presidential / democratic alliance against Russia and to take the perspective of a military defender, (3) DaveClubMember1 seems to ask for civic aid and engagement, (4) EdCutter1 seems to tweet about a mix of issues covered by the user 4 users (economy, government action, war crimes, etc.), (5) LateNighter5 tweets could also be about businesses and Republicans, and Seyahsm2 focuses on NATO, its border control, and hope. 7.3.1 Advanced: Word frequency comparisons You can plot the word frequencies against each other and get a real good overview, we need to transform the data in a way that every user has his/her own column with her/her word frequencies because we need to use these frequencies on a y- and y-axis. Therefore, we’ll use the pivot_wider function from the tidyr package that comes pre-installed with the tidyverse. frequency_wide &lt;- frequency %&gt;% select(user, word, freq) %&gt;% pivot_wider(names_from = user, values_from = freq) frequency_wide ## # A tibble: 111 × 7 ## word Seyahsm2 DaveClubMember1 common_sense54 BaloIreen EdCutter1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 america 9.88 NA NA NA NA ## 2 #prayforukrain NA 5.26 NA NA NA ## 3 #saveukrain NA 5.26 NA NA NA ## 4 #stopputin NA 5.26 NA NA NA ## 5 bio NA 5.26 NA NA NA ## 6 discord NA 5.26 NA NA NA ## 7 horribl NA 5.26 NA NA NA ## 8 imag NA 5.26 NA NA NA ## 9 join NA 5.26 NA NA NA ## 10 peopl NA 5.26 4.70 NA NA ## # … with 101 more rows, and 1 more variable: LateNighter5 &lt;dbl&gt; Now, let’s investigate which words are typical for DaveClubMember1 compared to common_sense54. frequency_wide %&gt;% filter(!is.na(DaveClubMember1)) %&gt;% # keep only words that are being used by both Twitter accounts filter(!is.na(common_sense54)) %&gt;% ggplot(aes(x=DaveClubMember1, y=common_sense54, label=word)) + # geom_point(alpha = 0.2, size = 4, position=position_jitter(h=0.15,w=0.15)) + geom_text(hjust=0, vjust=0, check_overlap = TRUE, position=position_jitter(h=0.25,w=0.25), size=3.0) + ylim(4.5,6.0) + xlim(4.5,6.0) + geom_abline(color = &quot;#67A9B6&quot;, na.rm=TRUE) Evaluation: Both accounts have mentioned all of the words in this plot at least once in their tweets. Words along the blue line are used approximately equally by both accounts, while words further away from the line are used significantly more frequently by one account than the other. Here, all terms are more characteristic of DaveClubMember1 than of common_sense54 because they are beneath the blue line. 7.3.2 Advanced: Word log odds So far, we’ve looked at how often a word is used compared to all the other words an account has posted (absolute &amp; relative frequencies). Next, we’ll look at how probable it is that a word was posted by a certain account (log odds). There is only one problem: measurement error. Probabilities are more accurate for words that are used frequently and less accurate for words that have been measured only a few times. A way to correct this measurement error is to use the tidylo package that creates weighted, i.e. corrected, log odds ratio based on an approach proposed by Monroe, Colaresi, and Quinn (2008). # Install / load the tidy log odds package to create weighted log odds if(!require(tidylo)) { install.packages(&quot;tidylo&quot;); require(tidylo) } #load / install+load tidylo word_lo &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) %&gt;% bind_log_odds(user, word, n, unweighted = TRUE) %&gt;% # mutate(prblty = (plogis(log_odds)*100)) %&gt;% arrange(desc(log_odds_weighted)) head(word_lo) ## # A tibble: 6 × 5 ## user word n log_odds log_odds_weighted ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Seyahsm2 america 32 2.00 11.3 ## 2 BaloIreen putin 10 1.49 8.49 ## 3 DaveClubMember1 #prayforukrain 30 1.52 8.34 ## 4 DaveClubMember1 #saveukrain 30 1.52 8.34 ## 5 DaveClubMember1 #stopputin 30 1.52 8.34 ## 6 DaveClubMember1 bio 30 1.52 8.34 Log odds ratio express the likelihood that a word comes from a certain account, compared to all other accounts. As a result, the highest log odds indicate words that are extremely distinctive for an account. Using a table, let’s compare the weighted log odds for some words across accounts. word_lo2 &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) %&gt;% bind_log_odds(user, word, n) %&gt;% select(-n) %&gt;% spread(user, log_odds_weighted, fill = 0) head(word_lo2) ## # A tibble: 6 × 7 ## word BaloIreen common_sense54 DaveClubMember1 EdCutter1 LateNighter5 Seyahsm2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 #ban… 2.29 0 0 0 0 0 ## 2 #pra… 0 0 8.34 0 0 0 ## 3 #sav… 0 0 8.34 0 0 0 ## 4 #sto… 0 0 8.34 0 0 0 ## 5 @fox… 0 2.53 0 0 0 0 ## 6 @gop 0 0 0 0 0 1.94 To get a better overview, let’s create a visualization of the 10 most distinctive words per account. word_lo %&gt;% group_by(user) %&gt;% arrange(desc(log_odds_weighted)) %&gt;% slice_head(n=10) %&gt;% ungroup %&gt;% mutate(word = reorder(word, log_odds_weighted)) %&gt;% ggplot(aes(word, log_odds_weighted, fill = user)) + geom_col(show.legend = FALSE) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#89BFC1&quot;,&quot;#67A9B6&quot;,&quot;#5495AD&quot;,&quot;#377099&quot;,&quot;#08333F&quot;)) + facet_wrap(~user, scales = &quot;free&quot;) + coord_flip() + labs(y = &quot;Log Odds Ratio&quot;, x=NULL) 7.4 Topic modeling In this section, we’ll use a combination of the tidytext package, the quanteda package, and the topicmodels package to create topic models with Latent Dirichlet allocation (LDA), one of the most prevalent topic modeling algorithms. LDA creates mixed-membership models, i.e., LDA assumes that every text contains a mix of different topics, e.g. a tweet can be 60% about TopicA (War crimes) and 40% about TopicB (Housing of refugees). Topics are defined by the mix of words that are associated with them. For example, the most common words in TopicA (War crimes) can be “massacre”, “soldiers”, and “brutal”. The most common words in TopicB (Housing of refugees) can be “volunteers”, “shelter”, and “children”. However, both topics can be mentioned in one single text, e.g., tweets about the brutal war crimes of Russian soldiers that force Ukrainian refugees to take their children and seek shelter in neighboring countries. LDA estimates the most common words in a topic and the most common topics in a text simultaneously. Image: Assigning topics to a document (Screenshot from: Chris Bail): Important hint: Both the quantedaand the topicmodels package use machine learning lingo. That is, words are called features and texts/tweets are called documents. The total sum of all documents is called a corpus. In the long run, you should get used to this lingo, so we will keep using it in this tutorial, too. 7.4.1 First steps If you haven’t already, please install / load the quanteda and topicmodels packages: # Install / load the quanteda package for data transformation if(!require(quanteda)) { install.packages(&quot;quanteda&quot;); require(quanteda) } #load / install+load quanteda # Install / load the topicmodels package for topic modeling if(!require(topicmodels)) { install.packages(&quot;topicmodels&quot;); require(topicmodels) } #load / install+load topicmodels Next, using the tidytext package, we will convert our tidy text data into a document-feature matrix (dfm) that the topicmodels package can understand and do calculations with. In a dfm… …rows represent the documents (= texts), …columns represent features (= unique words), …the cells represent the frequency with which a feature appears in a specific document. # Cast the tidy text data into a matrix (dfm) that topicmodels can use for calculation: dfm &lt;- data_tknzd %&gt;% select(tweet, text, word) %&gt;% count(tweet, word, sort = TRUE) %&gt;% cast_dfm(tweet, word, n) head(dfm) ## Document-feature matrix of: 6 documents, 1,718 features (99.18% sparse) and 0 docvars. ## features ## docs ukrain russia america russiaukrain @eucommiss @vonderleyen attack eu ## 231 3 1 0 0 0 0 0 0 ## 246 3 0 0 0 0 0 0 0 ## 614 3 1 0 0 0 0 0 0 ## 4485 1 3 0 0 0 0 0 0 ## 4527 2 3 0 0 0 0 0 0 ## 5431 3 1 0 0 0 0 0 0 ## features ## docs europ fellow ## 231 0 0 ## 246 0 0 ## 614 0 0 ## 4485 0 0 ## 4527 0 0 ## 5431 0 0 ## [ reached max_nfeat ... 1,708 more features ] Let’s check out the dimensions of this new document-feature matrix: paste(&quot;We are investigating &quot;, dim(dfm)[1], &quot; documents (tweets) and &quot;, dim(dfm)[2], &quot; features (unique words).&quot;) ## [1] &quot;We are investigating 9646 documents (tweets) and 1718 features (unique words).&quot; And let’s have a look at our top features: quanteda::topfeatures(dfm, 10) # this is a neat function from quanteda to investigate the top features in a dfm ## ukrain russia putin peopl war nato invas countri support world ## 9561 3777 1478 1114 1092 829 692 656 623 579 7.4.2 Model estimation: Number of topics As a researcher, you must first estimate the number of topics you anticipate to encounter across all documents (= the number of topics K in a corpus) before fitting an LDA model. If you estimate there are approximately 20 topics that Twitter accounts, for example, you’ll set K = 20 to extract 20 separate topics. The 20 topics are then extracted from the corpus based on the distribution of co-occurring words in each document. Choosing a good value for K is extremely important and consequential, since it will impact your results. Usually, small Ks produce very distinct, but generalizable topics, while high Ks produce overlapping themes, but are also more event- and issue-specific. Let’s create a topic model. More specifically, let’s create a six-topic LDA model using topicmodels. In practice, you would often use a higher K, but for our use case, three should suffice. lda &lt;- LDA(dfm, k = 3, control = list(seed = 123)) # the control argument uses a random number (123) to seed the assignment of topics to each word in the corpus (this helps to create reproducability in an otherwise random process!) Practice: Model estimation (Install +) Load the topicmodels package. Next, cast the tidy text data data_tknzd into a dfm that the topicmodels can use to calculate topic models. Finally, estimate an LDA-based topic model with 3 topics. 7.4.3 Inspect the topics 7.4.3.1 Word-topic probabilities To inspect the topics with the tidytext package, we need to tidy up our LDA model first, i.e., bring it back into a data format that works for tidy text analysis. # turn the lda model back into a tidy data frame: tidy_lda &lt;- tidy(lda, matrix = &quot;beta&quot;) %&gt;% # matrix = &quot;beta&quot; creates the word-topic probabilities rename(word = term) head(tidy_lda) ## # A tibble: 6 × 3 ## topic word beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 ukrain 0.152 ## 2 2 ukrain 0.148 ## 3 3 ukrain 0.0419 ## 4 1 russia 0.00401 ## 5 2 russia 0.111 ## 6 3 russia 0.0200 The new column, β (“beta”), shows the per-topic-per-word probabilities, i.e., the probability that the respective feature / word is being generated from the topic under examination. To put it another way, it’s the probability that a feature is common in a certain topic. The word-topic matrix is often used to analyze and label topics (i.e., using the features with the highest conditional probability for that topic). In summary, the word-topic matrix aids in the creation of topic-specific word lists. Let’s create a visualization of the 10 terms that are most common within each topic. tidy_lda %&gt;% group_by(topic) %&gt;% arrange(desc(beta)) %&gt;% slice_head(n=10) %&gt;% ungroup() %&gt;% ggplot(aes(reorder(word, beta), y=beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + ylim(0,0.4) + facet_wrap(~topic, scales=&quot;free&quot;, labeller = as_labeller(c(`1` = &quot;Topic 1&quot;, `2` = &quot;Topic 2&quot;, `3` = &quot;Topic 3&quot;))) + xlab(&quot;word&quot;) + coord_flip() As we can see, the topics are note really exclusive, i.e., have a lot of overlaps. This is a sign that we (a) need more preprocessing / text normalization and (b) need to adjust the number of K topics. However, topic modeling is usually more difficult for tweets than for news texts, so it’s unclear whether we’ll be able to significantly enhance the results anyway. Topic 1: May have the focus on a possible world war III or on the fact that the whole world should support Ukraine, but maybe not in a militaristic way because there are no words like “forces”, “troops” or “military” present. Maybe it focuses on both (an indicator that we need more topics than three)? Interestingly, this topic does not include “Russia” as a feature, so maybe it’s not about WWIII? Topic 2: This topic is directed at the NATO / U.S. America to end the conflict between Ukraine and Russia. Here, Russia is a prominent feature! Topic 3: This topic is hard to interpret, but it seems to be directed at U.S. president Joe Biden and asks for timely intervention? Amp could refer to Ukrainian war amps, i.e., soldiers who had to get body party amputated. So maybe this topic is about consequences for the life of Ukranians / Ukranian citizens? Now that we have a first impression about what the topics are dealing with, let’s take a look at the less frequent terms in each topic to check if our interpretation of the topics remains logical. Let’s have a look at the issues without the “top performers” ukrain, russia, and putin and look at the topics again. tidy_lda %&gt;% filter(word!=&quot;putin&quot;, word!=&quot;ukrain&quot;, word!=&quot;russia&quot;) %&gt;% group_by(topic) %&gt;% arrange(desc(beta)) %&gt;% slice_head(n=10) %&gt;% ungroup() %&gt;% ggplot(aes(x=reorder(word, beta), y=beta, fill = factor(topic))) + geom_col(show.legend = FALSE, stat = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + ylim(0,0.07) + facet_wrap(~topic, scales=&quot;free&quot;, labeller = as_labeller(c(`1` = &quot;Topic 1&quot;, `2` = &quot;Topic 2&quot;, `3` = &quot;Topic 3&quot;))) + xlab(&quot;word&quot;) + coord_flip() Excluding these top performers gives away more information. We might read the topics somewhat like this: Topic 1: The focus seems to be on war escalation (pro war). Here, tweets propose that the world (not U.S. America / NATO!) should join the fight? Topic 2: May have the focus on war deescalation (anti war). Here, tweets propose that U.S. America / NATO should stop the war? Topic 3: Maybe this topic has a focus on European border control that President Biden should provide? Still hard to interpret. Despite the fact that the topics are not mutually exclusive, they appear to be a reasonable approximation of a first tweet categorization (e.g., pro / anti war). The accuracy of this categorization must be confirmed, particularly by extensive / deep reading. Remember that topics are only suggestions that should be complemented with more sophisticated (often: qualitative) techniques. The high level of overlap across topics might possibly be attributed to the fact that the data set includes tweets from extremely comparable situations. All tweets were created on February 25 (the first day following the outbreak of Russian aggression against Ukraine) between 18:00 and 18:10 Paris time, i.e., all Tweets are, thematically speaking, about the Russian invasion of Ukraine. Naturally, the subjects of these tweets are fairly similar, which is to be anticipated in these circumstances. Thus, the three emerging topics serve (a little bit!) as frames of the same event since the conditions of their origin are so similar (namely the outbreak of Russian aggression). When understanding topics as frames, however, one should not go overboard (remember, e.g., that we are investigating multi-membership models)! See Nicholls &amp; Culpepper (2020) for a full explanation of why topics and frames are not the same thing, i.e., why topic modeling should not be used for framing analysis. Practice: Word-topic probabilities Now it’s your turn. Inspect the word-topic probabilities of the topics in the Kliemann data. To this end cast your lda model back into the tidy text format while calculating the beta values. After that, try to visualize your tidy data. Finally, evaluate the topic model that you see. What are the topics about? Are there any words that you would like to add to the stop word list, i.e., that you would like to exclude when rerunning the LDA analysis to produce better results? 7.4.3.2 Document-topic probabilities Now that we know which words are associated with what topics, we also want to know what documents (i.e., tweets) are associated with what topics. # again, turn the lda model back into a tidy data frame: tidy_lda2 &lt;- tidy(lda, matrix = &quot;gamma&quot;) # matrix = &quot;gamma&quot; creates the document-topic probabilities head(tidy_lda2) ## # A tibble: 6 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 231 1 0.340 ## 2 246 1 0.352 ## 3 614 1 0.331 ## 4 4485 1 0.320 ## 5 4527 1 0.327 ## 6 5431 1 0.327 The new column, γ (“gamma”), shows the per-document-per-topic probabilities, i.e., the proportion of words from that document that are generated from the topic under examination. To put it another way, it’s the probability that a topic is common in a certain document. The document-topic matrix is used to identify the top documents of a topic (i.e., using the documents with the highest probability for that topic) and to assign main topics to documents In summary, the word-topic matrix aids in the creation of document-specific topic lists. Let’s investigate which documents have the highest probability for Topic 2, the topic that seems to focus on war deescalation, i.e. stopping the fights. tidy_lda2 %&gt;% filter(topic == 2) %&gt;% arrange(desc(gamma)) ## # A tibble: 9,646 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9491 2 0.362 ## 2 31167 2 0.360 ## 3 37585 2 0.356 ## 4 36003 2 0.355 ## 5 45323 2 0.355 ## 6 4485 2 0.355 ## 7 18132 2 0.354 ## 8 12354 2 0.354 ## 9 33538 2 0.353 ## 10 26827 2 0.353 ## # … with 9,636 more rows Evaluation: 36.2% of document 9491, i.e. tweet No. 9491, are related to Topic2. This is also true for tweet No. 31167. Let’s have a look at both tweets and evaluate their word choice and full tweet text. data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 9491) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;Russia expects India to support it at the UNSC,when a resolution opposing the military Russian operation against Ukraine comes up for a vote on Friday evening (IST 1:30am saturday)- Babushkin Senior most Russian Diplomat in New Delhi \\n\\nIndian caught between devildeep blue sea&quot; data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 31167) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;@mfa_russia @NATO @RussianEmbFinla @Finland_OSCE @GenConFinSPb @Ulkoministerio @EmbFinMoscow @RusEmbNo @natomission_ru @USNATO @StateDept Let’s see, NATO hasn’t invaded anyone to force it to join while Russia has invaded Ukraine to force them to be friendly to Russia. Damn NATO sure is a threat to Russian security.&quot; Interesting! Both tweets are about diplomatic (instead of military) actions, i.e. which countries are in support of the Ukraine joining NATO and of the sovereignty of Ukraine. Of course, we should check more tweets to validate this, but it’s a good start. For comparison, let’s also look at documents that score high on Topic 1. tidy_lda2 %&gt;% filter(topic == 1) %&gt;% arrange(desc(gamma)) ## # A tibble: 9,646 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7805 1 0.356 ## 2 12781 1 0.355 ## 3 40030 1 0.354 ## 4 23553 1 0.354 ## 5 11080 1 0.353 ## 6 1551 1 0.353 ## 7 33308 1 0.353 ## 8 40417 1 0.353 ## 9 46920 1 0.353 ## 10 51830 1 0.353 ## # … with 9,636 more rows Both tweet No. 7805 and No. 37798 have a high share of topic 1. data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 7805) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;#Trump, who was impeached for withholding nearly $400 million in military aid from Ukraine, said &#39;this deadly Ukraine situation would never have happened&#39; if he were in office https://t.co/o258hgvhRw https://t.co/R7ivJcvg3s https://t.co/JWvVPzTohT&quot; data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 12781) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;@EmbassyofRussia @RussianEmbassy @mfa_russia @PMSimferopol @MID_Kaliningrad @RusEmbUSA @RTUKnews @russiabeyond @RusConsCapetown @Geostrat_ME @RusEmbEst A squad of Chechen special forces &#39;hunters&#39; has been unleashed in Ukraine to detain or KILL a set of specific Ukrainian officials......\\nhttps://t.co/PcuTQZeAri&quot; Thematically, these two tweets do not fit well together, but they both use a lot of brutal words, e.g., kill and deadly. Most likely this is the reason why they have been clustered together. Obviously, we should rerun this analysis with a greater number of topics because our current model does not differentiate well enough between different geographic regions and prominent actors. In most use cases, we would assume that 20-50 topics are a more accurate reflection of real-world conversations. Practice: Document-topic probabilities What tweets are associated with these topics? Cast the lda model into the tidy text format and calculate the gamma scores to investigate document-topic probabilities. Next, investigate the tweet that scores highest on the document-topic probabilities for Topic 1 and Topic 3. Do the tweets match your interpretation of the topics? 7.4.3.3 Assessment of the topics’ quality How do you assess the quality of proposed topic models? Use the approach recommended by Grimmer, Roberts &amp; Steward (2022, p. 152) when you want to assess the quality of your topic models. Read a random sample of documents allocated to a topic carefully, but keep in mind that documents are partial members of all topics. Therefore, the authors advise looking at documents that have a high proportion of words that are associated with the topic under examination. That is, one should create a small subset of documents where the largest portion of the document is associated with the particular topic of interest. Go over these documents to see what they have in common and whether the proposed topic makes sense from an organizational standpoint. Building on code provided by Ian T. Adams, we can create a beautiful overview of our extracted topics and the most common words that contribute to each topic: top_terms &lt;- tidy_lda %&gt;% arrange(desc(beta)) %&gt;% group_by(topic) %&gt;% slice_head(n=10) %&gt;% arrange(desc(beta)) %&gt;% select(topic, word) %&gt;% summarize(terms = list(word)) %&gt;% mutate(terms = map(terms, paste, collapse = &quot;, &quot;)) %&gt;% unnest(cols = c(terms)) gamma_terms &lt;- tidy_lda2 %&gt;% group_by(topic) %&gt;% summarize(gamma = mean(gamma)) %&gt;% arrange(desc(gamma)) %&gt;% left_join(top_terms, by = &quot;topic&quot;) %&gt;% mutate(topic = paste0(&quot;Topic &quot;, topic), topic = reorder(topic, gamma)) gamma_terms %&gt;% arrange(desc(gamma)) %&gt;% slice_head(n=10) %&gt;% mutate(topic = factor(topic, levels = c(&quot;Topic 1&quot;,&quot;Topic 2&quot;,&quot;Topic 3&quot;))) %&gt;% ggplot(aes(topic, gamma, label = terms, fill = topic)) + geom_col(show.legend = FALSE) + geom_text(hjust = 1.1, nudge_y = 0.0005, size = 3, color=&quot;white&quot;) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + coord_flip() + theme_bw() + theme(plot.title = element_text(size = 14)) + labs(x = NULL, y = expression(gamma), title = &quot;The Three Topics In The Ukrainian Twitter Data (25.02)&quot;, subtitle = &quot;With the top words that contribute to each topic&quot;) Or in a table format: library(knitr) gamma_terms %&gt;% mutate(topic = factor(topic, levels = c(&quot;Topic 1&quot;,&quot;Topic 2&quot;,&quot;Topic 3&quot;))) %&gt;% arrange(topic) %&gt;% select(topic, gamma, terms) %&gt;% kable(digits = 3, col.names = c(&quot;Topic&quot;, &quot;Expected topic proportion&quot;, &quot;Top 6 terms&quot;)) Topic Expected topic proportion Top 6 terms Topic 1 0.333 ukrain, putin, peopl, war, invas, support, invad, countri, world, attack Topic 2 0.333 ukrain, russia, putin, war, forc, countri, invas, america, nato, support Topic 3 0.333 ukrain, peopl, putin, russia, war, nato, world, presid, time, amp Final remark for research: While the quality of our three topics may appear satisfactory to you (but most likely not), we must remember that our choice of K was purely arbitrary. Post-hoc rationalization of this choice is inherently problematic. If possible, you should base your choice of K on previous literature and what other researchers have said about a suitable K for your corpus / your research question. If no such information is available, the searchK function of the stm package is your last resort, but it must always be accompanied by critical re-reading and examination of the topics and documents (e.g., semantic coherence and exclusivity of topics).5 Hands-on learning for practitioners: As you can see, topic modeling can turn into “reading tea leaves” (Chang et al., 2009). Depending on how many topics are extracted (number K) and how the data preprocessing was done, one can get very different results. Therefore, you should carefully review topic reports (e.g., the Meltwater’s Key Messages report). Please do not trust such automated analyses blindly. 7.4.4 STMs So far, we have only used the text of the tweet to estimate the topics. However, the creation date of a tweet could also be an important indicator of a topic, as some topics are more prevalent at certain points in time (the same applies to the author of a tweet, of course). While LDA has been the most famous topic modeling algorithm, there are currently a variety of similar techniques available, which all advance the field. One very popular technique is Structural Topic Modeling (STM), which is very similar to LDA, but uses meta data about documents (e.g., author name and creation date) to enhance word assignment to latent topics in the corpus. You can estimate STMs with the stm package. We will not discuss STMs in this tutorial due to time restrictions. However, the Additional tutorials section features two excellent external STM tutorials that you might want to have a look at. 7.5 Sentiment analysis The process of analyzing valence or emotions in text data is called sentiment analysis. One way to approach sentiment analysis is to understand text as a collection of individual words and assign each word with a negativity / emotion score. The text’s overall sentiment is interpreted as the sum of these individual scores. Professional, peer-reviewed “dictionaries” determine what score should be assigned to each individual word and they always look somewhat like this example: tidytext::get_sentiments(&quot;bing&quot;) ## # A tibble: 6,786 × 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative ## # … with 6,776 more rows If you want to use a dictionary, you might need to download and agree to their license first. Make sure that licenses match the requirements for your analysis (i.e., non-commercial use only). In the next section, we will apply different dictionaries to our Ukraine data. 7.5.1 Sentiment over time First, we need to create a data set that contains the word list of the bing dictionary. We will then add the sentiment evaluations of the bing word list to our tokenized data set: bing &lt;- tidytext::get_sentiments(&quot;bing&quot;) data_tknzd_bing &lt;- data_tknzd %&gt;% inner_join(bing) head(data_tknzd_bing) ## # A tibble: 6 × 6 ## time user tweet text word sentiment ## &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2022-02-25 18:10:59 peekay14 1 &quot;@vonderleyen @EU_Comm… usel… negative ## 2 2022-02-25 18:10:59 peekay14 1 &quot;@vonderleyen @EU_Comm… sever negative ## 3 2022-02-25 18:10:59 peekay14 1 &quot;@vonderleyen @EU_Comm… atta… negative ## 4 2022-02-25 18:10:59 koba19822012 12 &quot;@WorldNewsWWIII God b… bless positive ## 5 2022-02-25 18:10:59 nexta_tv 24 &quot;Zelensky stays in #Ky… misi… negative ## 6 2022-02-25 18:10:59 ananthoo1 31 &quot;What a speech! By Ukr… love positive Now we can estimate the sentiment of the tweets over time, i.e., see how the sentiment developed over time. data_tknzd %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(sentiment, time) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) %&gt;% ggplot(aes(x = time, y = sentiment)) + labs(title = &quot;Bing dictionary&quot;) + theme_bw() + geom_col(show.legend = FALSE) Overall, the tone of tweets is negative. Only a few tweets have a positive valence. Let’s check whether these results are characteristic for the bing dictionary and compare them to another dictionary, namely the afinn dictionary. Let’s load the afinn dictionary next. It’s included in the tidytext package, so you can agree to the licence and load it that way. I have added the afinn dictionary to a .csv to import it more effortlessly, so I’ll use this approach. afinn &lt;- read.csv(&quot;afinn_dictionary.csv&quot;) data_tknzd %&gt;% inner_join(afinn) %&gt;% group_by(time) %&gt;% summarize(sentiment = sum(value)) %&gt;% ggplot(aes(x = time, y = sentiment)) + labs(title = &quot;Afinn dictionary&quot;) + theme_bw() + geom_col(show.legend = FALSE) Even though both dictionaries produce similar results, we can see that afinn estimates a greater amount of positive tweets, while the bing dictionary estimates larger blocks of uninterrupted negative sentiment. This behavior of both dictionaries is known, for instance, see this quote: “Different lexicons for calculating sentiment give results that are different in an absolute sense but have similar relative trajectories . . . The AFINN lexicon gives the largest absolute values, with high positive values. The lexicon from Bing et al. has lower absolute values and seems to label larger blocks of contiguous positive or negative text.” (Silge &amp; Robinson, Link) Finally, other options include the nrc dictionary that can can estimate 1) negative and positive valence or 2) emotions such anger, sadness, joy, etc. For now, let’s import the valence word lists from nrcand repeat our analysis one last time. nrc &lt;- read.csv(&quot;nrc_dictionary.csv&quot;) nrc &lt;- nrc %&gt;% filter(sentiment == &quot;positive&quot; | sentiment == &quot;negative&quot;) data_tknzd %&gt;% inner_join(nrc) %&gt;% count(sentiment, time) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) %&gt;% ggplot(aes(x = time, y = sentiment)) + labs(title = &quot;NRC dictionary&quot;) + theme_bw() + geom_col(show.legend = FALSE) Obviously, the nrc dictionary estimates a much greater amount of positive tweets than bing and afinn. This is why you should handle dictionary-based approaches carefully. As Pipal et al. (2022) put it: “Many widely used tools for sentiment analysis have been developed by psychologists to measure emotions in free-form writing (such as in product reviews and micro blogs) in the form of pre-defined lexicons (e.g. Tausczik and Pennebaker 2010), yet they are often conveniently used to measure complex latent constructs in highly structured news articles and speeches despite the content-dependent nature of the tools (Puschmann and Powell 2018; Pipal, Schoonvelde, and Schumacher 2022). Different tools embody different methodological assumptions and particular contexts within which they are initially developed from, leading to diverging consequences across different tools.” To this day, most dictionaries cannot compete with manual annotation. Therefore you should manually validate the estimations before applying sentiment tools to your use case. For a very good comparison between dictionaries and manual annotation, see this article: Link. Two hands-on learnings: 1) Don’t trust sentiment analyses of pre-designed sentiment reports too much. If you don’t know how these sentiment scores were produced and how reliable those estimations are, you can’t evaluate their quality. 2) My suggestion is that you tailor your own dictionary to your specific use case (e.g., for your company some words might be positive that are negative for other companies) and validate it with manual coding until you are satisfied with its reliability. This is a one-time effort that can really pay-off, especially for smaller businesses. Practice: Sentiment over time Alright, we are ready to perform a sentiment analysis for the Kliemann data. Again, we cannot use the tidytext dictionaries because they were developed only for the English language. However, I’ve prepared a German dictionary for you that is based on this article by Christian Rauh. You can find the dictionary as a .csv on Moodle. Use it to analyze the sentiment of the Kliemann data over time. Hint: You will need to follow the sum(value) approach that I used on the afinn dictionary because the dictionary comes with a value column (metric variable) instead of a sentiment column (nominal variable). Tip for advanced students: One big problem with the above presented dictionary approaches is that they do not take into account sentence structure, but treat each sentence as a bag of words. For those dictionary-approaches, word order does not matter to transport sentiment. If you want to run more complex sentiment analyses in the future, try the vader package. vader is not just dictionary-based, but it is so a so-called “rule-based sentiment analysis tool”. This means that it takes into account sentence structure, e.g., negations. We all agree that “not cute” transports a negative sentiment, but the above presented dictionary approaches treat “not” as a stop word and evaluate “cute” as positive. 7.5.2 Sentiment over time periods You can define time periods and get the sentiment for each individual time frame. Let’s create three time periods (1 = first 3 min, 2 = 3rd to 6th min, 3 = whole 10 min) and compare the sentiment that both bing and afinn estimate for these three time periods. Here are the results for bing: data_tknzd %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% mutate(period = case_when( time &lt;= ymd_hms(&#39;2022-02-25 18:03:30&#39;) ~ &quot;Period 1&quot;, time &gt;= ymd_hms(&#39;2022-02-25 18:03:31&#39;) &amp; time &lt;= ymd_hms(&#39;2022-02-25 18:06:30&#39;) ~ &quot;Period 2&quot;, time &gt;= ymd_hms(&#39;2022-02-25 18:06:31&#39;) &amp; time &lt;= ymd_hms(&#39;2022-02-25 18:10:59&#39;) ~ &quot;Period 3&quot;)) %&gt;% count(period, sentiment) ## # A tibble: 6 × 3 ## period sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Period 1 negative 893 ## 2 Period 1 positive 502 ## 3 Period 2 negative 1635 ## 4 Period 2 positive 964 ## 5 Period 3 negative 2487 ## 6 Period 3 positive 1488 And here are the results for afinn: data_tknzd %&gt;% inner_join(afinn) %&gt;% mutate(period = case_when( time &lt;= ymd_hms(&#39;2022-02-25 18:03:30&#39;) ~ &quot;Period 1&quot;, time &gt;= ymd_hms(&#39;2022-02-25 18:03:31&#39;) &amp; time &lt;= ymd_hms(&#39;2022-02-25 18:06:30&#39;) ~ &quot;Period 2&quot;, time &gt;= ymd_hms(&#39;2022-02-25 18:06:31&#39;) &amp; time &lt;= ymd_hms(&#39;2022-02-25 18:10:59&#39;) ~ &quot;Period 3&quot;)) %&gt;% mutate(sentiment = case_when( value &lt; 0 ~ &quot;negative&quot;, value &gt; 0 ~ &quot;positive&quot; )) %&gt;% count(period, sentiment) ## # A tibble: 6 × 3 ## period sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Period 1 negative 1197 ## 2 Period 1 positive 731 ## 3 Period 2 negative 2112 ## 4 Period 2 positive 1283 ## 5 Period 3 negative 3219 ## 6 Period 3 positive 1999 Practice: Sentiment over time periods Now try to do a similar analysis with the Kliemann data and investigate the sentiment for each individual month starting and ending with the 6th day of this month. Our data from stretches from 6th of May to 3rd of July, so you have to include these three days of July in the June time period. 7.5.3 Sentiment of individual users Now we can calculate the sentiment for each individual user. Let’s try it. data_tknzd_bing &lt;- data_tknzd_bing %&gt;% count(user, sentiment, time) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) data_tknzd_bing %&gt;% filter(user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) ## # A tibble: 63 × 5 ## user time negative positive sentiment ## &lt;chr&gt; &lt;dttm&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 BaloIreen 2022-02-25 18:02:33 1 2 1 ## 2 BaloIreen 2022-02-25 18:02:51 1 2 1 ## 3 BaloIreen 2022-02-25 18:02:59 1 2 1 ## 4 BaloIreen 2022-02-25 18:03:07 1 2 1 ## 5 BaloIreen 2022-02-25 18:03:21 1 2 1 ## 6 BaloIreen 2022-02-25 18:03:34 1 2 1 ## 7 BaloIreen 2022-02-25 18:03:44 1 2 1 ## 8 BaloIreen 2022-02-25 18:09:01 1 2 1 ## 9 BaloIreen 2022-02-25 18:09:28 1 2 1 ## 10 BaloIreen 2022-02-25 18:06:08 0 1 1 ## # … with 53 more rows …and turn it into a visualization of our most active users over time: data_tknzd_bing %&gt;% group_by(user, time) %&gt;% filter(user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% ggplot(aes(x = time, y = sentiment, fill = user)) + geom_col(show.legend = FALSE) + labs(title = &quot;Bing: The sentiment of the most active users&quot;) + theme_bw() + facet_wrap(~user) According to the bing dictionary the sentiment of our most active twitter users was positive and stayed positive over the 10-minute time frame that we are currently looking at. Positive sentiment while talking about the outbreak of war? Those are odd results. Maybe the positive sentiment is characteristic of more active users? data_tknzd %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(sentiment, time) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) %&gt;% ggplot(aes(x = time, y = sentiment)) + labs(title = &quot;Bing dictionary&quot;) + theme_bw() + geom_col(show.legend = FALSE) Overall, the tone of tweets is negative, which implies that the most active users are characterized by a more positive sentiment. That’s an interesting result and might be an indicator that very active accounts are trying to push the public opinion in some specific direction. We could run more in-depth analyses of more users and longer time-periods to prove that, but for now we want to focus on alternative dictionaries. Knowing this, let’s check how similar the results are for our most active users. data_tknzd %&gt;% inner_join(afinn) %&gt;% group_by(user, time) %&gt;% summarize(sentiment = sum(value)) %&gt;% filter(user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% ggplot(aes(x = time, y = sentiment, fill = user)) + geom_col(show.legend = FALSE) + labs(title = &quot;Afinn: The sentiment of the most active users&quot;) + theme_bw() + facet_wrap(~user) That looks good, both bing and afinn come to similar conclusions, i.e., the most active users publish tweets with a positive sentiment. The dictionary approaches that treat sentiment as the sum of the individual negativity/emotion ratings of words are consistent with each other. 7.6 Take-Aways Text normalization: You can perform the entire preprocessing of your documents using the tidy text approach (tokenization, stop words, removal, etc.). Word freqencies &amp; Log Odds: The tidy text approach can also be used to calculate word frequencies and log odds to identify the most common word per author, etc. Topic modeling: Topic models are mixed-membership models, i.e., they assume that every text contains a mix of different topics. The best known topic modeling algorithm is LDA. K: Number of topics to be estimated. Word-topic matrix: Aids in the creation of topic-specific word lists. Document-topic matrix: Aids in the creation of document-specific topic lists. 7.7 Additional tutorials You still have questions? The following tutorials, books, &amp; papers may help you: Text as Data by V. Hase, Tutorial 13 Topic Modeling by C. Bail Text Mining in R. A Tidy Approach by Silge &amp; Robinson (2017) To disable this feature, use the to_lower = FALSE option.↩︎ This function takes a range of K values, calculates topic models for each value of K, and then generates goodness-of-fit measurements that indicate what value for K fits the data best.↩︎ "],["solutions.html", "Solutions Solutions for Exercise 1 Solutions for Exercise 2 Solutions for Exercise 3 Solutions for Exercise 4 Solutions for Exercise 5 Solutions for Exercise 6", " Solutions This is where you’ll find solutions for all of the tutorials. Solutions for Exercise 1 Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What panels are part of RStudio? Solution: source (x) console (x) packages, files &amp; plots (x) 2. How do you activate R packages after you have installed them? Solution: library() (x) 3. How do you create a vector in R with elements 1, 2, 3? Solution: c(1,2,3) (x) 4. Imagine you have a vector called ‘vector’ with 10 numeric elements. How do you retrieve the 8th element? Solution: vector[8] (x) 5. Imagine you have a vector called ‘hair’ with 5 elements: brown, black, red, blond, other. How do you retrieve the color ‘blond’? Solution: hair[4] (x) Task 2 Create a numeric vector with 8 values and assign the name age to the vector. First, display all elements of the vector. Then print only the 5th element. After that, display all elements except the 5th. Finally, display the elements at the positions 6 to 8. Solution: age &lt;- c(65,52,73,71,80,62,68,87) age ## [1] 65 52 73 71 80 62 68 87 age[5] ## [1] 80 age[-5] ## [1] 65 52 73 71 62 68 87 age[6:8] ## [1] 62 68 87 Task 3 Create a non-numeric, i.e. character, vector with 4 elements and assign the name eye_color to the vector. First, print all elements of this vector to the console. Then have only the value in the 2nd element displayed, then all values except the 2nd element. At the end, display the elements at the positions 2 to 4. Solution: eye_color &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;brown&quot;, &quot;other&quot;) eye_color ## [1] &quot;blue&quot; &quot;green&quot; &quot;brown&quot; &quot;other&quot; eye_color[2] ## [1] &quot;green&quot; eye_color[-2] ## [1] &quot;blue&quot; &quot;brown&quot; &quot;other&quot; eye_color[2:4] ## [1] &quot;green&quot; &quot;brown&quot; &quot;other&quot; Task 4 Get the “data_tutorial2.csv” from Moodle ( 4. Mai material folder ) and put it into the folder that you want to use as working directory. Set your working directory and load the data into R by saving it into a source object called data. Note: This time, it’s a csv that is actually separated by commas, not by semicolons. Solution: setwd(&quot;C:/Users/LaraK/Documents/IPR/&quot;) data &lt;- read.csv(&quot;data_tutorial2.csv&quot;, header = TRUE) Task 5 Now, print only the age column to the console. Use the $ operator first. Then try to achieve the same result using the subsetting operators, i.e. []. Solution: data$age # first version ## [1] 20 25 29 22 25 26 26 27 8 26 27 26 25 27 29 26 21 23 24 26 data[,2] # second version ## [1] 20 25 29 22 25 26 26 27 8 26 27 26 25 27 29 26 21 23 24 26 Task 6 Print only the first 6 age numbers to the console. Use the $ operator first. Then try to achieve the same result using the subsetting operators, i.e. []. Solution: data$age[1:6] # first version ## [1] 20 25 29 22 25 26 data[1:6,2] # second version ## [1] 20 25 29 22 25 26 Solutions for Exercise 2 Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What are the main characteristics of tidy data? Solution: Every observation is a row. (x) 2. What are dplyr functions? Solution: mutate() (x) 3. How can you sort the eye_color of Star Wars characters from Z to A? Solution: starwars_data %&gt;% arrange(desc(eye_color)) (x) starwars_data %&gt;% select(eye_color) %&gt;% arrange(desc(eye_color)) 4. Imagine you want to recode the height of the these characters. You want to have three categories from small and medium to tall. What is a valid approach? Solution: starwars_data %&gt;% mutate(height = case_when(height&lt;=150~\"small\",height&lt;=190~\"medium\",height&gt;190~\"tall\")) 5. Imagine you want to provide a systematic overview over all hair colors and what species wear these hair colors frequently (not accounting for the skewed sampling of species)? What is a valid approach? Solution: starwars_data %&gt;% group_by(hair_color, species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) Task 2 Now it’s you turn. Load the starwars data like this: library(dplyr) # to activate the dplyr package starwars_data &lt;- starwars # to assign the pre-installed starwars data set (dplyr) into a source object in our environment How many humans are contained in the starwars data overall? (Hint: use summarize(count = n()) or count())? Solution: You can use summarize(count = n()): starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% summarize(count = n()) ## # A tibble: 1 × 1 ## count ## &lt;int&gt; ## 1 35 Alternatively, you can use the count() function: starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% count(species) ## # A tibble: 1 × 2 ## species n ## &lt;chr&gt; &lt;int&gt; ## 1 Human 35 Task 3 How many humans are contained in starwars by gender? Solution: You can use summarize(count = n()): starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% group_by(species, gender) %&gt;% summarize(count = n()) ## # A tibble: 2 × 3 ## # Groups: species [1] ## species gender count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Human feminine 9 ## 2 Human masculine 26 Alternatively, you can use the count() function: starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% count(species, gender) ## # A tibble: 2 × 3 ## species gender n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Human feminine 9 ## 2 Human masculine 26 Task 4 What is the most common eye_color among Star Wars characters? (Hint: use arrange())__ Solution: starwars_data %&gt;% group_by(eye_color) %&gt;% summarize(count = n()) %&gt;% arrange(desc(count)) ## # A tibble: 15 × 2 ## eye_color count ## &lt;chr&gt; &lt;int&gt; ## 1 brown 21 ## 2 blue 19 ## 3 yellow 11 ## 4 black 10 ## 5 orange 8 ## 6 red 5 ## 7 hazel 3 ## 8 unknown 3 ## 9 blue-gray 1 ## 10 dark 1 ## 11 gold 1 ## 12 green, yellow 1 ## 13 pink 1 ## 14 red, blue 1 ## 15 white 1 Task 5 What is the average mass of Star Wars characters that are not human and have yellow eyes? (Hint: remove all NAs)__ Solution: starwars_data %&gt;% filter(species != &quot;Human&quot; &amp; eye_color==&quot;yellow&quot;) %&gt;% summarize(mean_mass = mean(mass, na.rm=TRUE)) ## # A tibble: 1 × 1 ## mean_mass ## &lt;dbl&gt; ## 1 74.1 Task 6 Compare the mean, median, and standard deviation of mass for all humans and droids. (Hint: remove all NAs)__ Solution: starwars_data %&gt;% filter(species==&quot;Human&quot; | species==&quot;Droid&quot;) %&gt;% group_by(species) %&gt;% summarize(M = mean(mass, na.rm = TRUE), Med = median(mass, na.rm = TRUE), SD = sd(mass, na.rm = TRUE) ) ## # A tibble: 2 × 4 ## species M Med SD ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Droid 69.8 53.5 51.0 ## 2 Human 82.8 79 19.4 Task 7 Create a new variable in which you store the mass in gram. Add it to the data frame. Solution: starwars_data &lt;- starwars_data %&gt;% mutate(gr_mass = mass*1000) starwars_data %&gt;% select(name, species, mass, gr_mass) ## # A tibble: 87 × 4 ## name species mass gr_mass ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 77 77000 ## 2 C-3PO Droid 75 75000 ## 3 R2-D2 Droid 32 32000 ## 4 Darth Vader Human 136 136000 ## 5 Leia Organa Human 49 49000 ## 6 Owen Lars Human 120 120000 ## 7 Beru Whitesun lars Human 75 75000 ## 8 R5-D4 Droid 32 32000 ## 9 Biggs Darklighter Human 84 84000 ## 10 Obi-Wan Kenobi Human 77 77000 ## # … with 77 more rows Solutions for Exercise 3 Task 1 Try to reproduce this plot with dplyr and ggplot2. (Hint: You can hide the legend by adding theme(legend.position = \"none\") to your plot.) Solution: data %&gt;% mutate(sex = case_when( sex == 0 ~ &quot;Female&quot;, sex == 1 ~ &quot;Male&quot;)) %&gt;% mutate(Party = case_when( partyid == 1 ~ &quot;Democrat&quot;, partyid == 2 ~ &quot;Independent&quot;, partyid == 3 ~ &quot;Republican&quot;)) %&gt;% ggplot(aes(x=Party,y=negemot, fill=Party)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Climate change attitudes of U.S. partisans by gender&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_wrap(~sex, nrow=2) Task 2 Now, try to reproduce this graph. (Hint: You will need to recode the ideology variable in a way that higher values represent stronger attitudes, independent of partisanship.) Solution: data &lt;- data %&gt;% mutate(ideology_ext = case_when( ideology == 1 ~ 4, ideology == 2 ~ 3, ideology == 3 ~ 2, ideology == 4 ~ 1, ideology == 5 ~ 2, ideology == 6 ~ 3, ideology == 7 ~ 4)) %&gt;% mutate(sex = case_when( sex == 0 ~ &quot;Female&quot;, sex == 1 ~ &quot;Male&quot;)) %&gt;% mutate(Party = case_when( partyid == 1 ~ &quot;Democrat&quot;, partyid == 2 ~ &quot;Independent&quot;, partyid == 3 ~ &quot;Republican&quot;)) data %&gt;% ggplot(aes(x=Party,y=ideology_ext, fill=Party)) + geom_boxplot() + theme_bw() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Ideological extremity of U.S. partisans by gender&quot;, y = &quot;Ideological extremity&quot;) + facet_wrap(~sex) Task 3 Can you make a chart that breaks down the relationship between age, negative emotions about climate change, and ideological extremity for the different sexes AND parties? Solution 1: data %&gt;% ggplot(aes(x=age,y=negemot, size=ideology_ext, color = Party)) + geom_point() + scale_size(range = c(0.3, 3), name = &quot;Ideological extremity&quot;) + # You can&#39;t guess the exact value that I&#39;ve used here. Just use whatever looks good for you and comes close to the solution. theme_bw() + labs(title = &quot;Relationship between age, climate change attitudes, \\n and ideological extremity&quot;, x = &quot;Age&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_wrap(~sex, nrow=2) Solution 2: Alternatively, you might enjoy this look that you can create with facet_grid(): data %&gt;% ggplot(aes(x=age,y=negemot, size=ideology_ext, color = Party)) + geom_point() + scale_size(range = c(0.3, 3), name = &quot;Ideological extremity&quot;) + theme_bw() + labs(title = &quot;Relationship between age, climate change attitudes, and ideological extremity&quot;, x = &quot;Age&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_grid(vars(sex), vars(Party)) Solution 3: Or even this look, also done with facet_grid(): data %&gt;% ggplot(aes(x=age,y=negemot, size=ideology_ext, color = Party)) + geom_point() + scale_size(range = c(0.3, 3), name = &quot;Ideological extremity&quot;) + theme_bw() + labs(title = &quot;Relationship between age, climate change attitudes, and ideological extremity&quot;, x = &quot;Age&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_grid(~sex + Party) Solutions for Exercise 4 This exercise was created because some students asked to get some more practice. You don’t have to work through it, but it can certainly help you with the graded assignment. In this exercise, we will work with the mtcars data that comes pre-installed with dplyr. library(tidyverse) data &lt;- mtcars # To make the data somewhat more interesting, let&#39;s set a few values to missing values: data$wt &lt;- na_if(data$wt, 4.070) data$mpg &lt;- na_if(data$mpg, 22.8) Task 1 Check the data set for missing values (NAs) and delete all observations that have missing values. Solution: You can solve this by excluding NAs in every single column: data &lt;- data %&gt;% # we&#39;ll now only keep observations that are NOT NAs in the following variables (remember that &amp; = AND): filter(!is.na(mpg) &amp; !is.na(cyl) &amp; !is.na(disp) &amp; !is.na(hp) &amp; !is.na(drat) &amp; !is.na(wt) &amp; !is.na(qsec) &amp; !is.na(vs) &amp; !is.na(am) &amp; !is.na(gear) &amp; !is.na(carb)) Alternatively, excluding NAs from the entire data set works, too, but you have not learned the na_omit()function in the tutorials: data &lt;- data %&gt;% na.omit() Task 2 Let’s transform the weight wt of the cars. Currently, it’s given as Weight in 1000 lbs. I guess you are not used to lbs, so try to mutate wt to represent Weight in 1000 kg. 1000 lbs = 453.59 kg, so we will need to divide by 2.20. Similarly, I think that you are not very familiar with the unit Miles per gallon of the mpg variable. Let’s transform it into Kilometer per liter. 1 m/g = 0.425144 km/l, so again divide by 2.20. Solution: data &lt;- data %&gt;% mutate(wt = wt/2.20) data &lt;- data %&gt;% mutate(mpg = mpg/2.20) Task 3 Now we want to group the weight of the cars in three categories: light, medium, heavy. But how to define light, medium, and heavy cars, i.e., at what kg should you put the threshold? A reasonable approach is to use quantiles (see Tutorial: summarize() [+ group_by()]). Quantiles divide data. For example, the 75% quantile states that exactly 75% of the data values are equal or below the quantile value. The rest of the values are equal or above it. Use the lower quantile (0.25) and the upper quantile (0.75) to estimate two values that divide the weight of the cars in three groups. What are these values? Solution: data %&gt;% summarize(UQ_wt= quantile(wt, 0.75), LQ_wt= quantile(wt, 0.25)) ## UQ_wt LQ_wt ## 1 1.622727 1.190909 75% of all cars weigh 1.622727* 1000kg or less and 25% of all cars weigh 1.190909* 1000kg or less. Task 4 Use the values from Task 3 to create a new variable wt_cat that divides the cars in three groups: light, medium, and heavy cars. Solution: data &lt;- data %&gt;% mutate(wt_cat = case_when( wt &lt;= 1.190909 ~ &quot;light car&quot;, wt &lt; 1.622727 ~ &quot;medium car&quot;, wt &gt;= 1.622727 ~ &quot;heavy car&quot;)) Task 5 How many light, medium, and heavy cars are part of the data? Solution: You can solve this with the summarize(count = n() function: data %&gt;% group_by(wt_cat) %&gt;% summarize(count = n()) ## # A tibble: 3 × 2 ## wt_cat count ## &lt;chr&gt; &lt;int&gt; ## 1 heavy car 9 ## 2 light car 7 ## 3 medium car 13 9 heavy cars, 13 medium cars, and 7 light cars. Alternatively, you can also use the count() function: data %&gt;% count(wt_cat) ## wt_cat n ## 1 heavy car 9 ## 2 light car 7 ## 3 medium car 13 Task 6 Now sort this count of the car weight classes from highest to lowest. Solution: data %&gt;% group_by(wt_cat) %&gt;% summarize(count = n()) %&gt;% arrange(desc(count)) ## # A tibble: 3 × 2 ## wt_cat count ## &lt;chr&gt; &lt;int&gt; ## 1 medium car 13 ## 2 heavy car 9 ## 3 light car 7 Task 7 Make a scatter plot to indicate how many km per liter (mpg) a car can drive depending on its weight (wt). Facet the plot by weight class (wt_cat). Try to hide the plot legend (you have learned that in another exercise). data %&gt;% mutate(wt_cat = factor(wt_cat, levels = c(&quot;light car&quot;, &quot;medium car&quot;, &quot;heavy car&quot;))) %&gt;% ggplot(aes(x=wt, y=mpg, color=wt_cat)) + geom_point() + theme_bw() + scale_color_manual(values = c(&quot;#7b3294&quot;, &quot;#84798a&quot;, &quot;#008837&quot;)) + # optional command, choose your own beautiful colors for the graph theme(legend.position = &quot;none&quot;) + labs(title = &quot;Relationship between car weight and achieved kilometers per liter&quot;, x=&quot;Weight in 1000kg&quot;, y=&quot;km/l&quot;) + facet_wrap(~wt_cat) Task 8 Recreate the diagram from Task 7, but exclude all cars that weigh between 1.4613636 and 1.5636364 *1000kg from it. Solution: data %&gt;% filter(wt &lt; 1.4613636 | wt &gt; 1.5636364) %&gt;% mutate(wt_cat = factor(wt_cat, levels = c(&quot;light car&quot;, &quot;medium car&quot;, &quot;heavy car&quot;))) %&gt;% ggplot(aes(x=wt, y=mpg, color=wt_cat)) + geom_point() + theme_bw() + scale_color_manual(values = c(&quot;#7b3294&quot;, &quot;#84798a&quot;, &quot;#008837&quot;)) + # optional command, choose your own beautiful colors for the graph theme(legend.position = &quot;none&quot;) + labs(title = &quot;Relationship between car weight and achieved kilometers per liter&quot;, x=&quot;Weight in 1000kg&quot;, y=&quot;km/l&quot;) + facet_wrap(~wt_cat) Why would we use data %&gt;% filter(wt &lt; 1.4613636 | wt &gt; 1.5636364) instead of data %&gt;% filter(wt &gt; 1.4613636 | wt &lt; 1.5636364)? Let’s look at the resulting data sets when you apply those filters to compare them: data %&gt;% select(wt) %&gt;% filter(wt &lt; 1.4613636 | wt &gt; 1.5636364) ## wt ## Mazda RX4 1.1909091 ## Mazda RX4 Wag 1.3068182 ## Valiant 1.5727273 ## Duster 360 1.6227273 ## Merc 240D 1.4500000 ## Merc 450SL 1.6954545 ## Merc 450SLC 1.7181818 ## Cadillac Fleetwood 2.3863636 ## Lincoln Continental 2.4654545 ## Chrysler Imperial 2.4295455 ## Fiat 128 1.0000000 ## Honda Civic 0.7340909 ## Toyota Corolla 0.8340909 ## Toyota Corona 1.1204545 ## Dodge Challenger 1.6000000 ## Camaro Z28 1.7454545 ## Pontiac Firebird 1.7477273 ## Fiat X1-9 0.8795455 ## Porsche 914-2 0.9727273 ## Lotus Europa 0.6877273 ## Ford Pantera L 1.4409091 ## Ferrari Dino 1.2590909 ## Maserati Bora 1.6227273 ## Volvo 142E 1.2636364 The resulting table does not include any cars that weigh between 1.4613636 and 1.5636364. But if you use data %&gt;% filter(wt &gt; 1.4613636 | wt &lt; 1.5636364)… data %&gt;% select(wt) %&gt;% filter(wt &gt; 1.4613636 | wt &lt; 1.5636364) ## wt ## Mazda RX4 1.1909091 ## Mazda RX4 Wag 1.3068182 ## Hornet 4 Drive 1.4613636 ## Hornet Sportabout 1.5636364 ## Valiant 1.5727273 ## Duster 360 1.6227273 ## Merc 240D 1.4500000 ## Merc 280 1.5636364 ## Merc 280C 1.5636364 ## Merc 450SL 1.6954545 ## Merc 450SLC 1.7181818 ## Cadillac Fleetwood 2.3863636 ## Lincoln Continental 2.4654545 ## Chrysler Imperial 2.4295455 ## Fiat 128 1.0000000 ## Honda Civic 0.7340909 ## Toyota Corolla 0.8340909 ## Toyota Corona 1.1204545 ## Dodge Challenger 1.6000000 ## AMC Javelin 1.5613636 ## Camaro Z28 1.7454545 ## Pontiac Firebird 1.7477273 ## Fiat X1-9 0.8795455 ## Porsche 914-2 0.9727273 ## Lotus Europa 0.6877273 ## Ford Pantera L 1.4409091 ## Ferrari Dino 1.2590909 ## Maserati Bora 1.6227273 ## Volvo 142E 1.2636364 … cars that weigh between 1.4613636 and 1.5636364 are still included! But why? The filter()function always keeps cases based on the criteria that you provide. In plain English, my solution code says the following: Take my dataset “data” and keep only those cases where the weight variable wt is less than 1.4613636 OR larger than 1.5636364. Put differently, the solution code says: Delete all cases that are greater than 1.4613636 but are also less than 1.5636364. The wrong code, on the other hand, says: Take my dataset “data” and keep only those cases where the weight variable wt is greater than 1.4613636 OR smaller than 1.5636364. This is ALL the data because all your cases will be greater than 1.4613636 OR smaller than 1.5636364. You are not excluding any cars. Solutions for Exercise 5 First, load the library stringr and create a new car vector: library(stringr) cars &lt;- c(&quot;VW&quot;, &quot;Mercedes-Benz&quot;, &quot;BMW&quot;, &quot;Audi&quot;, &quot;Opel&quot;, &quot;Skoda&quot;, &quot;Ford&quot;, &quot;Seat&quot;, &quot;Cupra&quot;, &quot;Hyundai&quot;, &quot;Renault&quot;, &quot;Fiat&quot;, &quot;Toyota&quot;, &quot;Peugeot&quot;, &quot;Volvo&quot;) Task 1 You have a list of the most popular cars in Europe (according to car buyers in 2021). Marketers believe that the secret to product sales is a catchy name, often between 3 and 4 letters. Is this true? Try to find out how long the brands’ names are. Solution: str_length(cars) ## [1] 2 13 3 4 4 5 4 4 5 7 7 4 6 7 5 7 of the 15 cars are between 2 and 4 characters long. This is not the majority, but pretty substantial. Catchy, short names might be a good choice for an automobile brand. Task 2 Some brands cheat: They actually have very long names and use acronyms to make their brand more catchy. Let’s expose them! Replace all acronyms with the real brand name, i.e., VW = Volkswagen and BMW = Bayerische Motoren Werke. Solution: cars &lt;- str_replace(cars, &quot;VW&quot;, &quot;Volkswagen&quot;) cars &lt;- str_replace(cars, &quot;BMW&quot;, &quot;Bayerische Motoren Werke&quot;) cars ## [1] &quot;Volkswagen&quot; &quot;Mercedes-Benz&quot; ## [3] &quot;Bayerische Motoren Werke&quot; &quot;Audi&quot; ## [5] &quot;Opel&quot; &quot;Skoda&quot; ## [7] &quot;Ford&quot; &quot;Seat&quot; ## [9] &quot;Cupra&quot; &quot;Hyundai&quot; ## [11] &quot;Renault&quot; &quot;Fiat&quot; ## [13] &quot;Toyota&quot; &quot;Peugeot&quot; ## [15] &quot;Volvo&quot; Task 3 There is proof that some sounds are cacophonic, such as voiced fricatives (/f/,/v/,/s/,/z/,/h/), while others are regarded euphonic, such as vowels (/a/, /e/) and liquids (/l/, /r/) (see here). Marketers sometimes recommend not to use cacophonic sounds in brand names. Find all car brands that use cacophonic sounds despite these warnings. Solution: str_detect(cars,&quot;[fvszhFVSZH]&quot;) ## [1] TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## [13] FALSE FALSE TRUE str_subset(cars,&quot;[fvszhFVSZH]&quot;) ## [1] &quot;Volkswagen&quot; &quot;Mercedes-Benz&quot; ## [3] &quot;Bayerische Motoren Werke&quot; &quot;Skoda&quot; ## [5] &quot;Ford&quot; &quot;Seat&quot; ## [7] &quot;Hyundai&quot; &quot;Fiat&quot; ## [9] &quot;Volvo&quot; Six car brands do not use cacophonic sounds, whereas nine do. It would appear that cacophonic sounds are not exactly being avoided. Task 4 How many cacophonic sounds are used in these brand names, i.e., are there any brands that use (at least) two cacophonic sounds? Solution: str_count(cars, &quot;[fvszhFVSZH]&quot;) ## [1] 2 2 2 0 0 1 1 1 0 1 0 1 0 0 2 Four brands, Volkswagen, Mercedes-Benz, Bayerische Motoren Werke, and Volvo use two cacophonic sounds. Task 5 Which brand names begin with a cacophonic sound? Solution: str_detect(cars, &quot;^[FVSZH]&quot;) ## [1] TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## [13] FALSE FALSE TRUE str_subset(cars, &quot;^[FVSZH]&quot;) ## [1] &quot;Volkswagen&quot; &quot;Skoda&quot; &quot;Ford&quot; &quot;Seat&quot; &quot;Hyundai&quot; ## [6] &quot;Fiat&quot; &quot;Volvo&quot; Seven car brands start with a cacophonic sound: Volkswagen, Skoda, Ford, Seat, Hyundai, Fiat, and Volvo. Task 6 Which brand names end with a cacophonic sound? Solution: str_detect(cars, &quot;[fvszh]$&quot;) ## [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE str_subset(cars, &quot;[fvszh]$&quot;) ## [1] &quot;Mercedes-Benz&quot; Only one car brand ends with a cacophonic sound: Mercedes-Benz. Task 7 Let’s correct those cacophonic brand names by replacing all cacophonic sounds with a liquid sound. Let’s replace all inner cacophonic sounds with /l/ and all cacophonic sounds that start a brand name with /R/. Hint: To keep things simple, use the command to replace inner cacophonic sounds 2 times. Solution: cars &lt;- str_replace(cars, &quot;[fvszh]+&quot;, &quot;l&quot;) cars &lt;- str_replace(cars, &quot;[fvszh]+&quot;, &quot;l&quot;) cars &lt;- str_replace(cars, &quot;[FVSZH]+&quot;, &quot;R&quot;) cars ## [1] &quot;Rolklwagen&quot; &quot;Mercedel-Benl&quot; ## [3] &quot;Bayerilcle Motoren Werke&quot; &quot;Audi&quot; ## [5] &quot;Opel&quot; &quot;Rkoda&quot; ## [7] &quot;Rord&quot; &quot;Reat&quot; ## [9] &quot;Cupra&quot; &quot;Ryundai&quot; ## [11] &quot;Renault&quot; &quot;Riat&quot; ## [13] &quot;Toyota&quot; &quot;Peugeot&quot; ## [15] &quot;Rollo&quot; That worked! Mercedel-Benl and Rollo are definitely my personal favorites. Solutions for Exercise 6 Task 1: Tokenization Load the Kliemann data into RStudio. Use the tutorial code and to set the encoding. After loading the Kliemann data keep only the time, user, and full_text column. Next, try to tokenize the data. As an extra, delete all tokens that are mentions of other twitter users (i.e., that start with an @-smybol). Solution: # First: Load and shorten data data &lt;- read.csv(&quot;Kliemann-full-tweets.csv&quot;, encoding = &quot;UTF-8&quot;) data_short &lt;- data %&gt;% select(time, user, full_text) Sys.setenv(TZ=&quot;UTC&quot;) data_short &lt;- data_short %&gt;% mutate(time = lubridate::ymd_hms(time)) %&gt;% tibble() # Second: Tokenize remove_reg &lt;- &quot;&amp;amp;|&amp;lt;|&amp;gt;&quot; data_tknzd &lt;- data_short %&gt;% mutate(tweet = row_number()) %&gt;% filter(!str_detect(full_text, &quot;^RT&quot;)) %&gt;% mutate(text = str_remove_all(full_text, remove_reg)) %&gt;% unnest_tokens(word, full_text, token = &quot;tweets&quot;) %&gt;% filter(!str_detect(word, &quot;^[:digit:]+$&quot;)) %&gt;% filter(!str_detect(word, &quot;^http&quot;)) %&gt;% filter(!str_detect(word, &quot;^(\\\\+)+$&quot;)) %&gt;% filter(!str_detect(word, &quot;^(\\\\+)+(.)+$&quot;)) %&gt;% filter(!str_detect(word, &quot;^(\\\\-)+$&quot;)) %&gt;% filter(!str_detect(word, &quot;^(\\\\-)+(.)+$&quot;)) %&gt;% filter(!str_detect(word, &quot;^(.)+(\\\\+)+$&quot;)) %&gt;% filter(!str_detect(word, &quot;^@&quot;)) # a new line for the task: remove all words that are mentions Task 2: Stop word removal Now it’s your turn. The Kliemann data is in German, so you can’t use the tidytext stop word list, which is meant for English text only. So install and load the ‘stopwords’ package that allows you to create a dataframe that contains German stop words by using this command: stop_word_german &lt;- data.frame(word = stopwords::stopwords(\"de\"), stringsAsFactors = FALSE). Create your German stop word list and use it to remove stop words from your tokens. Solution: # First: install and load the stopwords package if(!require(stopwords)) { install.packages(&quot;stopwords&quot;); require(stopwords) } #load / install+load stopwords # Second: create a stop word list stop_word_german &lt;- data.frame(word = stopwords::stopwords(&quot;de&quot;), stringsAsFactors = FALSE) # Third: remove German stop words from tokens data_tknzd &lt;- data_tknzd %&gt;% filter(!word %in% stop_word_german$word) Optional solution: If you want to add additional stop words to your stop word list, you can use this solution instead. I would recommend using it, because German stop word lists are often not as advanced as English stop word lists. In addition, they need to be tailored for specific text types, such as colloquial German: # First: install and load the stopwords package if(!require(stopwords)) { install.packages(&quot;stopwords&quot;); require(stopwords) } #load / install+load stopwords # Second: create a stop word list stop_word_german &lt;- data.frame(word = stopwords::stopwords(&quot;de&quot;), stringsAsFactors = FALSE) # Optional: Here you can insert your own stop words, if the German list seems too short for you (231 words against 1149 in English) stop_word_german &lt;- stop_word_german %&gt;% add_row(word = &quot;beim&quot;) %&gt;% add_row(word = &quot;and&quot;) %&gt;% add_row(word = &quot;mehr&quot;) %&gt;% add_row(word = &quot;ganz&quot;) %&gt;% add_row(word = &quot;fast&quot;) %&gt;% add_row(word = &quot;klar&quot;) %&gt;% add_row(word = &quot;mal&quot;) %&gt;% add_row(word = &quot;dat&quot;) %&gt;% add_row(word = &quot;biste&quot;) %&gt;% add_row(word = &quot;schon&quot;) %&gt;% add_row(word = &quot;gell&quot;) %&gt;% add_row(word = &quot;dass&quot;) %&gt;% add_row(word = &quot;seit&quot;) %&gt;% add_row(word = &quot;ja&quot;) %&gt;% add_row(word = &quot;wohl&quot;) %&gt;% add_row(word = &quot;gar&quot;) %&gt;% add_row(word = &quot;ne&quot;) %&gt;% add_row(word = &quot;sone&quot;) %&gt;% add_row(word = &quot;dar&quot;) %&gt;% add_row(word = &quot;ahja&quot;) %&gt;% add_row(word = &quot;eher&quot;) %&gt;% add_row(word = &quot;naja&quot;) %&gt;% add_row(word = &quot;yes&quot;) %&gt;% add_row(word = &quot;pls&quot;) %&gt;% add_row(word = &quot;halt&quot;) %&gt;% add_row(word = &quot;hast&quot;) %&gt;% add_row(word = &quot;hat&quot;) %&gt;% add_row(word = &quot;wurde&quot;) %&gt;% add_row(word = &quot;wurden&quot;) %&gt;% add_row(word = &quot;wurdest&quot;) %&gt;% add_row(word = &quot;war&quot;) %&gt;% add_row(word = &quot;warst&quot;) %&gt;% add_row(word = &quot;gib&quot;) %&gt;% add_row(word = &quot;gibst&quot;) %&gt;% add_row(word = &quot;gibt&quot;) %&gt;% add_row(word = &quot;entweder&quot;) %&gt;% add_row(word = &quot;beinahe&quot;) %&gt;% add_row(word = &quot;ganz&quot;) %&gt;% add_row(word = &quot;ganze&quot;) %&gt;% add_row(word = &quot;ganzen&quot;)%&gt;% add_row(word = &quot;hey&quot;) %&gt;% add_row(word = &quot;eigentlich&quot;) %&gt;% add_row(word = &quot;gerade&quot;) %&gt;% add_row(word = &quot;irgendwie&quot;) # Third: remove German stop words from tokens data_tknzd &lt;- data_tknzd %&gt;% filter(!word %in% stop_word_german$word) Task 3: Lemmatizing &amp; stemming Please stem the Kliemann data with the PorterStemmer. Since we are working with German data, you’ll have to add the option language = \"german\" to the wordStem() function. Solution: # First: import the Porter stemmer library(SnowballC) # Second: apply the PorterStemmer to your tokens data_tknzd &lt;- data_tknzd %&gt;% mutate(word = wordStem(word, language = &quot;german&quot;)) Task 4: Pruning Please, try the prune function for yourself. Prune the Kliemann data and remove 1) words that occur in less than 0.01% of all tweets and 2) words that occur in more than 95% of all tweets. Solution: data_tknzd &lt;- prune(data_tknzd, tweet, word, text, user, 0.001, 0.95) Task 5: Model estimation (Install +) Load the topicmodels package. Next, cast the tidy text data data_tknzd into a dfm that the topicmodels can use to calculate topic models. Finally, estimate an LDA-based topic model with 3 topics. Solution: # First, load the `topicmodels` package library(topicmodels) # Second, cast the tidy data set into a dfm dfm &lt;- data_tknzd %&gt;% select(tweet, text, word) %&gt;% count(tweet, word, sort = TRUE) %&gt;% cast_dfm(tweet, word, n) # Third, estimate the LDA models with 3 topics lda &lt;- LDA(dfm, k = 3, control = list(seed = 123)) Task 6: Word-topic probabilities Now it’s your turn. Inspect the word-topic probabilities of the topics in the Kliemann data. To this end cast your lda model back into the tidy text format while calculating the beta values. After that, try to visualize your tidy data. Finally, evaluate the topic model that you see. What are the topics about? Are there any words that you would like to add to the stop word list, i.e., that you would like to exclude when rerunning the LDA analysis to produce better results? Solution: # First, cast back your data into the tidy format tidy_lda &lt;- tidy(lda, matrix = &quot;beta&quot;) %&gt;% # matrix = &quot;beta&quot; creates the word-topic probabilities rename(word = term) # Second, visualize your data with ggplot tidy_lda %&gt;% group_by(topic) %&gt;% arrange(desc(beta)) %&gt;% slice_head(n=10) %&gt;% ungroup() %&gt;% ggplot(aes(reorder(word, beta), y=beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + ylim(0,0.4) + facet_wrap(~topic, scales=&quot;free&quot;, labeller = as_labeller(c(`1` = &quot;Topic 1&quot;, `2` = &quot;Topic 2&quot;, `3` = &quot;Topic 3&quot;))) + xlab(&quot;word&quot;) + coord_flip() Topic 1 might deal with Fynn Kliemann’s communication strategy, i.e. whether his reaction videos were a good or a bad idea to stop the shitstorm. Topic 2 is not clearly identifiable, but it seems to deal with more recent events, namely that Fynn Kliemann partially retracted his guilty plea in June. Instead, he blamed the “woke left” and its “cancel culture” for the shitstorm against himself and his company Kliemannsland GmbH. Topic 3 clearly deals with the mask fraud scandal involving Fynn Kliemann and the fair-trade store he runs, which is called “ODERSO”. Looking at the results, it might be a good idea to exclude different versions of the name “Fynn Kliemann” prior to the LDA estimation by adding these names to the stop word list. Currently, the names are obscuring the results. As a quick (and dirty) fix, we can just exclude them from our visualization: tidy_lda %&gt;% filter(word != &quot;kliemann&quot;, word != &quot;fynn&quot;, word != &quot;#fynnkliemann&quot;, word != &quot;#kliemann&quot;) %&gt;% group_by(topic) %&gt;% arrange(desc(beta)) %&gt;% slice_head(n=10) %&gt;% ungroup() %&gt;% ggplot(aes(reorder(word, beta), y=beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + ylim(0,0.1) + facet_wrap(~topic, scales=&quot;free&quot;, labeller = as_labeller(c(`1` = &quot;Topic 1&quot;, `2` = &quot;Topic 2&quot;, `3` = &quot;Topic 3&quot;))) + xlab(&quot;word&quot;) + coord_flip() Task 7: Document-topic probabilities What tweets are associated with these topics? Cast the lda model into the tidy text format and calculate the gamma scores to investigate document-topic probabilities. Next, investigate the tweet that scores highest on the document-topic probabilities for Topic 1 and Topic 3. Do the tweets match your interpretation of the topics? # First, turn the lda model back into a tidy data frame and calculate the gamma scores: tidy_lda2 &lt;- tidy(lda, matrix = &quot;gamma&quot;) # Second, look at the tweets that have the highest probability for Topic 1 tidy_lda2 %&gt;% filter(topic == 1) %&gt;% arrange(desc(gamma)) ## # A tibble: 15,094 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7311 1 0.368 ## 2 15603 1 0.363 ## 3 9080 1 0.361 ## 4 12830 1 0.358 ## 5 4943 1 0.358 ## 6 5316 1 0.357 ## 7 2189 1 0.357 ## 8 339 1 0.356 ## 9 14259 1 0.356 ## 10 10009 1 0.355 ## # … with 15,084 more rows data_tknzd %&gt;% select(tweet, text) %&gt;% filter(tweet == 10277) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;@SaveKuzeyali Lasst uns Kuzey Ali retten!\\n\\n#Selenskyj #RyanGosling #Kliemann #Brückentag #Hubschrauber #LuisaNeubauer #capitalcom #FOMC #Merkel &quot; data_tknzd %&gt;% select(tweet, text) %&gt;% filter(tweet == 752) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;Was jetzt passiert:\\nFynn Kliemann wird \\n-Kurz und knapp ankündigen dass er wohl ‚den Boden unter den Füßen verloren hat in diesem Business‘ und sich erstmal (aus Social Media) zurück ziehen wird, um sein altes Ich zu suchen \\n-Erklären dass er schon länger Therapie macht (Mitleid)&quot; data_tknzd %&gt;% select(tweet, text) %&gt;% filter(tweet == 9128) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;@AnnKat__ @Andrerseits Würde ich nicht pauschal so sagen. Bei der Kliemann-Situation habe ich das hier öfters gelesen. Fand es auch nicht lustig. Aber vielleicht wollte diejenigen nur äußeren, dass ihnen a) die Situation und/oder b) der Typ völlig egal sind. Socialmedia eben.&quot; # Third, look at the tweets that have the highest probability tidy_lda2 %&gt;% filter(topic == 3) %&gt;% arrange(desc(gamma)) ## # A tibble: 15,094 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 15016 3 0.363 ## 2 8575 3 0.359 ## 3 1839 3 0.358 ## 4 7928 3 0.358 ## 5 13698 3 0.357 ## 6 8327 3 0.356 ## 7 4053 3 0.355 ## 8 12762 3 0.355 ## 9 8023 3 0.355 ## 10 3069 3 0.355 ## # … with 15,084 more rows data_tknzd %&gt;% select(tweet, text) %&gt;% filter(tweet == 7002) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;Der Youtuber Fynn Kliemann soll nach Recherchen des ZDF Magazin Royale unter anderem an fragwürdigen Masken-Deals im großen Stil beteiligt gewesen sein. Mehr dazu: https://t.co/PSD1vXuMjm&quot; data_tknzd %&gt;% select(tweet, text) %&gt;% filter(tweet == 2131) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;Eine Recherche des „ZDF Magazin Royale\\&quot; zeigt: Fynn Kliemann war an schmutzigen Maskendeals beteiligt. In seinen Antworten auf die Fragen der Redaktion log er offenbar, um das zu vertuschen. &quot; data_tknzd %&gt;% select(tweet, text) %&gt;% filter(tweet == 4903) %&gt;% filter(row_number()==1) %&gt;% pull(text) ## [1] &quot;@der_Pimon @filmmagde @RaykAnders @janboehm ... und zu dem Zeitpunkt, war Fynn Kliemann noch gar nicht an Global Tactics beteiligt. Das erwähnt Böhmermann und das Team aber nicht, obwohl es bekannt ist. Durch das Weglassen dieser Details wird der Internet-Mob aufgestachelt. ...&quot; Task 8: Sentiment over time Alright, we are ready to perform a sentiment analysis for the Kliemann data. Again, we cannot use the tidytext dictionaries because they were developed only for the English language. However, I’ve prepared a German dictionary for you that is based on this article by Christian Rauh. You can find the dictionary as a .csv on Moodle. Use it to analyze the sentiment of the Kliemann data over time. Hint: You will need to follow the sum(value) approach that I used on the afinn dictionary because the dictionary comes with a value column (metric variable) instead of a sentiment column (nominal variable). sentiWS &lt;- read.csv(&quot;SentiWS_Rauh_dictionary.csv&quot;) data_tknzd %&gt;% inner_join(sentiWS) %&gt;% group_by(time) %&gt;% summarize(sentiment = sum(value)) %&gt;% ggplot(aes(x = time, y = sentiment)) + geom_line() + labs(title = &quot;SentiWS (Rauh) dictionary&quot;) + theme_bw() + geom_col(show.legend = FALSE) This graph looks as if the debate around Kliemann was polarized, i.e., that negative buzz also produced positive counterarguing (at least most of the time). Task 9: Sentiment over time periods Now try to do a similar analysis with the Kliemann data and investigate the sentiment for each individual month starting and ending with the 6th day of this month. Our data from stretches from 6th of May to 3rd of July, so you have to include these three days of July in the June time period. data_tknzd %&gt;% inner_join(sentiWS) %&gt;% mutate(period = case_when( time &lt;= ymd_hms(&#39;2022-06-05 23:59:59&#39;) ~ &quot;May&quot;, time &gt;= ymd_hms(&#39;2022-06-06 00:00:00&#39;) ~ &quot;June&quot;)) %&gt;% mutate(sentiment = case_when( value &lt; 0 ~ &quot;negative&quot;, value &gt; 0 ~ &quot;positive&quot; )) %&gt;% count(period, sentiment) ## # A tibble: 4 × 3 ## period sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 June negative 1388 ## 2 June positive 2237 ## 3 May negative 2330 ## 4 May positive 4885 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
